{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HybridRAG: Integrating Knowledge Graphs and Vector Retrieval\\nAugmented Generation for Efficient Information Extraction\\nBhaskarjitSarmah BenikaHall RohanRao\\nbhaskarjit.sarmah@blackrock.com bhall@nvidia.com rohrao@nvidia.com\\nBlackRock,Inc. NVIDIA NVIDIA\\nGurugram,India SantaClara,CA,USA SantaClara,CA,USA\\nSunilPatel StefanoPasquali DhagashMehta\\nsupatel@nvidia.com stefano.pasquali@blackrock.com dhagash.mehta@blackrock.com\\nNVIDIA BlackRock,Inc. BlackRock,Inc.\\nSantaClara,CA,USA NewYork,NY,USA NewYork,NY,USA\\nABSTRACT AlthoughLLMshavesubstantialpotentialinfinancialapplica-\\nExtractionandinterpretationofintricateinformationfromunstruc- tions,therearenotablechallengesinusingpre-trainedmodelsto\\nturedtextdataarisinginfinancialapplications,suchasearnings extractinformationfromfinancialdocumentsoutsidetheirtraining\\ncalltranscripts,presentsubstantialchallengestolargelanguage datawhilealsoreducinghallucination[7,8].Financialdocuments\\nmodels (LLMs) even using the current best practices to use Re- typicallycontaindomain-specificlanguage,multipledataformats,\\ntrievalAugmentedGeneration(RAG)(referredtoasVectorRAG anduniquecontextualrelationshipsthatgeneralpurpose-trained\\ntechniqueswhichutilizevectordatabasesforinformationretrieval) LLMsdonothandlewell.Inaddition,extractingconsistentand\\nduetochallengessuchasdomainspecificterminologyandcomplex coherentinformationfrommultiplefinancialdocumentscanbe\\nformatsofthedocuments.Weintroduceanovelapproachbased challengingduetovariationsinterminology,format,andcontext\\nonacombination,calledHybridRAG,oftheKnowledgeGraphs acrossdifferenttextualsources.Thespecializedterminologyand\\n(KGs)basedRAGtechniques(calledGraphRAG)andVectorRAG complexdataformatsinfinancialdocumentsmakeitdifficultfor\\ntechniquestoenhancequestion-answer(Q&A)systemsforinfor- modelstoextractmeaningfulinsights,inturn,causinginaccurate\\nmationextractionfromfinancialdocumentsthatisshowntobe predictions,overlookedinsights,andunreliableanalysis,which\\ncapableofgeneratingaccurateandcontextuallyrelevantanswers. ultimatelyhindertheabilitytomakewell-informeddecisions.\\nUsing experiments on a set of financial earning call transcripts Current approaches to mitigate these issues include various\\ndocumentswhichcomeintheformofQ&Aformat,andhence Retrieval-AugmentedGeneration(RAG)techniques[9],whichaim\\nprovideanaturalsetofpairsofground-truthQ&As,weshowthat toimprovetheperformanceofLLMsbyincorporatingrelevant\\nHybridRAGwhichretrievescontextfrombothvectordatabaseand retrievaltechniques.VectorRAG(thetraditionalRAGtechniques\\nKGoutperformsbothtraditionalVectorRAGandGraphRAGindi- thatarebasedonvectordatabases)focusesonimprovingNatural\\nviduallywhenevaluatedatboththeretrievalandgenerationstages Language Processing (NLP) tasks by retrieving relevant textual\\nintermsofretrievalaccuracyandanswergeneration.Theproposed informationtosupportthegenerationtasks.VectorRAGexcelsin\\ntechniquehasapplicationsbeyondthefinancialdomain. situationswherecontextfromrelatedtextualdocumentsiscrucial\\nforgeneratingmeaningfulandcoherentresponses[9‚Äì11].RAG-\\n1 INTRODUCTION basedmethodsensuretheLLMsgeneraterelevantandcoherent\\nresponses that are aligned with the original input query. How-\\nForthefinancialanalyst,itiscrucialtoextractandanalyzeinfor-\\never,forfinancialdocuments,theseapproacheshavesignificant\\nmationfromunstructureddatasourceslikenewsarticles,earnings\\nchallengesasastandalonesolution.Forinstance,traditionalRAG\\nreports,andotherfinancialdocumentstohaveatleastsomechance\\nsystems often use paragraph-level chunking techniques, which\\ntobeonthebettersideofpotentialinformationasymmetry.These\\nassumethetextinthosedocumentsareuniforminlength.This\\nsourcesholdvaluableinsightsthatcanimpactinvestmentdecisions,\\napproachneglectsthehierarchicalnatureoffinancialstatements\\nmarketpredictions,andoverallsentiment.However,traditional\\nandcanresultinthelossofcriticalcontextualinformationforan\\ndataanalysismethodsstruggletoeffectivelyextractandutilize\\naccurateanalysis[12,13].Duetothecomplexitiesinanalyzingfi-\\nthisinformationduetoitsunstructurednature.Largelanguage\\nnancialdocuments,thequalityoftheLLMretrieved-contextfrom\\nmodels(LLMs)[1‚Äì4]haveemergedaspowerfultoolsforfinancial\\navastandheterogeneouscorpuscanbeinconsistent,leadingto\\nservicesandinvestmentmanagement.Theirabilitytoprocessand\\ninaccuraciesandincompleteanalyses.Thesechallengesdemon-\\nunderstandvastamountsoftextualdatamakestheminvaluable\\nstratetheneedformoresophisticatedmethodsthatcaneffectively\\nfortaskssuchassentimentanalysis,markettrendpredictions,and\\nintegrateandprocessthedetailedanddomain-specificinformation\\nautomatedreportgeneration.Specifically,extractinginformation\\nfoundinfinancialdocuments,ensuringmorereliableandaccurate\\nfromannualreportsandotherfinancialdocumentscangreatlyen-\\nresultsforinformeddecision-making.\\nhancetheefficiencyandaccuracyoffinancialanalysts[5].Arobust\\nKnowledgegraphs(KGs)[14]mayprovideadifferentpointof\\ninformationextractionsystemcanhelpanalystsquicklygather\\nviewtolookingatthefinancialdocumentswherethedocuments\\nrelevantdata,identifymarkettrends,andmakeinformeddecisions,\\nleadingtobetterinvestmentstrategiesandriskmanagement[6].\\n4202\\nguA\\n9\\n]LC.sc[\\n1v84940.8042:viXraSarmahetal.\\nareviewedasacollectionoftripletsofentitiesandtheirrelation- financialcalltranscriptsofthecompaniesincludedintheNifty-50\\nshipsasdepictedinthetextofthedocuments.KGshavebecome indexwhichisanIndianstockmarketindexthatrepresentsthe\\na pivotal technology in data management and analysis, provid- weightedaverageof50ofthelargestIndiancompanieslistedon\\ningastructuredwaytorepresentknowledgethroughentitiesand theNationalStockExchange1.\\nrelationshipsandhavebeenwidelyadoptedinvariousdomains,\\nincludingsearchengines,recommendationsystems,andbiomedical 2 METHODOLOGY\\nresearch[15‚Äì17].TheprimaryadvantageofKGsliesintheirability InthisSection,weprovidedetailsoftheproposedmethodology\\ntoofferastructuredrepresentation,whichfacilitatesefficientquery- byfirstdiscussingdetailsofVectorRAG,thenmethodologiesof\\ningandreasoning.However,buildingandmaintainingKGsand constructingKGsfromgivendocumentsandourproposedmethod-\\nintegratingdatafromdifferentsources,suchasdocuments,news ologyofGraphRAGandfinallytheHybridGraphtechnique.\\narticles, and other external sources, into a coherent knowledge\\ngraphposessignificantchallenges. 2.1 VectorRAG\\nThefinancialservicesindustryhasrecognizedthepotentialof\\nThetraditionalRAG[9]processbeginswithaquerythatisrelated\\nKGsinenhancingdataintegrationofheterogeneousdatasources,\\ntotheinformationpossessedwithinexternaldocument(s)thatare\\nriskmanagement,andpredictiveanalytics[18?‚Äì21].FinancialKGs\\nnotapartofthetrainingdatasetfortheLLM.Thisqueryisusedto\\nintegratevariousfinancialdatasourcessuchasmarketdata,fi-\\nsearchanexternalrepository,suchasavectordatabaseorindexed\\nnancialreports,andnewsarticles,creatingacomprehensiveview\\ncorpus,tofetchrelevantdocumentsorpassagescontaininguseful\\noffinancialentitiesandtheirrelationships.Thisunifiedviewim-\\ninformation.Theseretrieveddocumentsarethenfedbackintothe\\nprovestheaccuracyandcomprehensivenessoffinancialanalysis,\\nLLMasadditionalcontext.Hence,inturn,forthegivenquery,the\\nfacilitatesriskmanagementbyidentifyinghiddenrelationships,\\nlanguagemodelgeneratesaresponsebasednotonlyonitsinter-\\nandsupportsadvancedpredictiveanalyticsformoreaccuratemar-\\nnaltrainingdatabutalsobyincorporatingtheretrievedexternal\\nketpredictionsandinvestmentdecisions.However,handlinglarge\\ninformation.Thisintegrationensuresthatthegeneratedcontentis\\nvolumesoffinancialdataandcontinuouslyupdatingtheknowledge\\ngroundedinmorerecentandverifiabledata,improvingtheaccu-\\ngraphtoreflectthedynamicnatureoffinancialmarketscanbe\\nracyandcontextualrelevanceoftheresponses.Bycombiningthe\\nchallengingandresource-intensive.\\nretrievalofexternalinformationwiththegenerativecapabilities\\nGraphRAG(Graph-basedRetrieval-AugmentedGeneration)[22‚Äì\\noflargelanguagemodels,RAGenhancestheoverallqualityand\\n27]isanovelapproachthatleveragesknowledgegraphs(KGs)to\\nreliabilityofthegeneratedtext.\\nenhancetheperformanceofNLPtaskssuchasQ&Asystems.By\\nIntraditionalVectorRAG,thegivenexternaldocumentsaredi-\\nintegratingKGswithRAGtechniques,GraphRAGenablesmore\\nvidedintomultiplechunksbecauseofthelimitationofcontextsize\\naccurateandcontext-awaregenerationofresponsesbasedonthe\\nofthelanguagemodel.Thosechunksareconvertedintoembed-\\nstructuredinformationextractedfromfinancialdocuments.But\\ndingsusinganembeddingsmodelandthenstoredintoavector\\nGraphRAGgenerallyunderperformsinabstractiveQ&Atasksor\\ndatabase.Afterthat,theretrievalcomponentperformsasimilarity\\nwhenthereisnotexplicitentitymentionedinthequestion.\\nsearchwithinthevectordatabasetoidentifyandrankthechunks\\nInthepresentwork,weproposeacombinationofVectorRAGand\\nmostrelevanttothequery.Thetop-rankedchunksareretrieved\\nGraphRAG,calledHybridRAG,toretrievetherelevantinformation\\nandaggregatedtoprovidecontextforthegenerativemodel.\\nfromexternaldocumentsforagivenquerytotheLLMthatbrings\\nThen,thegenerativemodeltakesthisretrievedcontextalong\\nadvantagesofboththeRAGstogethertoprovidedemonstrably\\nwiththeoriginalqueryandsynthesizesaresponse.Thus,itmerges\\nmoreaccurateanswerstothequeries.\\nthereal-timeinformationfromtheretrievedchunkswithitspre-\\nexistingknowledge,ensuringthattheresponseisbothcontextually\\n1.1 PriorWorkandOurContribution relevantanddetailed.\\nTheschematicdiagraminFigure1providesdetailsonthepart\\nVectorRAGhasbeenextensivelyinvestigatedintherecentyears\\nofRAGthatgeneratesvectordatabasefromgivenexternaldocu-\\nandfocusesonenhancingNLPtasksbyretrievingrelevanttextual\\nmentsinthetraditionalVectorRAGmethodologywherewealso\\ninformationtosupportgenerationprocesses[9‚Äì11,26].However,\\nincludeexplicitreferenceofmetadata[8].Section4.2willprovide\\ntheeffectivenessoftheretrievalmechanismacrossmultipledocu-\\nimplementationdetailsforourexperiments.\\nmentsandlongercontextsposesasignificantchallengeinextract-\\ningrelevantresponses.GraphRAGcombinesthecapabilitiesofKGs 2.2 KnowledgeGraphConstruction\\nwithRAGtoimprovetraditionalNLPtasks[23‚Äì25].Withinour\\nAKGisastructuredrepresentationofreal-worldentities,their\\nimplementationsofbothVectorRAGGraphRAGtechniques,we\\nattributes,andtheirrelations,usuallystoredinagraphdatabase\\nexplicitlyaddinformationonthemetadataofthedocumentsthat\\noratripletstore,i.e.,aKGconsistsofnodesthatrepresententities\\nisalsoshowntoimprovetheperformanceofVectorRAG[8].\\nandedgesthatrepresentrelations,aswellaslabelsandattributes\\nTothebestofourknowledgethepresentworkisthefirstwork\\nforboth.AgraphtripletisabasicunitofinformationinaKG,\\nthatproposesaRAGapproachthatishybridofbothVectorRAG\\nconsistingofasubject,apredicate,andanobject.\\nandGraphRAGanddemonstratesitspotentialofmoreeffective\\nInmostmethodologiestobuildaKGfromgivendocuments,\\nanalysisandutilizationoffinancialdocumentsbyleveragingthe\\nthreemainstepsareinvolved:knowledgeextraction,knowledge\\ncombinedstrengthsofVectorRAGandGraphRAG.Wealsoutilizea\\nnovelground-truthQ&Adatasetextractedfrompubliclyavailable 1https://www.nseindia.com/products/content/equities/indices/nifty_50.htmHybridRAG:IntegratingKnowledgeGraphsandVectorRetrievalAugmentedGenerationforEfficientInformationExtraction\\ntriplet extraction pipeline. The second tier of our LLM chain is\\ndedicatedtoentityextractionandrelationshipidentification.\\nBoththestepsareexecutedusingcarefullyperformedprompt\\nengineeringonapre-trainedLLM.Adetaileddiscussiononimple-\\nmentationofthemethodologywillbeprovidedinSection4.1\\n2.3 GraphRAG\\nKGbasedRAG[22],orGraphRAG,alsobeginswithaquerybased\\nontheuser‚ÄôsinputsameasVectorRAG.Themaindifferencebe-\\ntweenVectorRAGandGraphRAGliesintheretrievalpart.The\\nqueryhereisnowusedtosearchtheKGtoretrieverelevantnodes\\n(entities)andedges(relationships)relatedtothequery.Asubgraph,\\nFigure1:Aschematicdiagramdescribingthevectordatabase whichconsistsoftheserelevantnodesandedges,isextractedfrom\\ncreationofaRAGapplication. thefullKGtoprovidecontext.Thissubgraphisthenintegrated\\nwiththelanguagemodel‚Äôsinternalknowledge,byencodingthe\\ngraphstructureintoembeddingsthatthemodelcaninterpret.The\\nlanguagemodelusesthiscombinedcontexttogenerateresponses\\nimprovement,andknowledgeadaptation[28].Here,wedonotuse thatareinformedbyboththestructuredinformationfromtheKG\\nknowledgeadaptationandtreattheKGsasstaticgraphs. anditspre-trainedknowledge.Crucially,whenrespondingtouser\\nKnowledgeExtraction:-Thisstepaimstoextractstructuredin- queriesaboutaparticularcompany,weleveragedthemetadata\\nformationfromunstructuredorsemi-structureddata,suchastext, informationtoselectivelyfilterandretrieveonlythosedocument\\ndatabases,andexistingontologies.Themaintasksinthisstepare segmentspertinenttothequeriedcompany[8].Thisintegration\\nentityrecognition,relationshipextraction,andco-referencereso- helpsensurethatthegeneratedoutputsareaccurate,contextually\\nlution.Entityrecognitionandrelationshipextractiontechniques relevant,andgroundedinverifiableinformation.\\nusetypicalNLPtaskstoidentifyentitiesandtheirrelationships AschematicdiagramoftheretrievalmethodologyofGraphRAG\\nfromtextualsources[29].Coreferenceresolutionidentifiesand is given in Figure 2. Here we first write a prompt to clean the\\nconnectsdifferentreferencesofthesameentity,keepingcoherence dataandthenwriteanotherpromptinthesecondstagetocreate\\nwithintheknowledgegraph.Forexample,ifthetextreferstoa knowledgetripletsalongwithmetadata.Itwillbecoveredinmore\\ncompanyasboth\"thecompany\"and\"it\",coreferenceresolution detailinsection4.1\\ncanlinkthesementionstothesameentitynodeinthegraph.\\nKnowledgeImprovement:-Thisstepaimstoenhancethequality\\nandcompletenessoftheKGbyremovingredundanciesandaddress-\\ninggapsintheextractedinformation.Theprimarytasksinthis\\nstepareKGcompletionandfusion.KGcompletiontechniqueinfers\\nmissingentitiesandrelationshipswithinthegraphusingmeth-\\nodssuchaslinkpredictionandentityresolution.Linkprediction\\npredictstheexistenceandtypeofarelationbetweentwoentities\\nbasedonthegraphstructureandfeatures,whileentityresolution\\nmatchesandmergesdifferentrepresentationsofthesameentity\\nfromdifferentsources.\\nKnowledgefusioncombinesinformationfrommultiplesources\\ntocreateacoherentandunifiedKG.Thisinvolvesresolvingconflicts\\nand redundancies among the sources, such as contradictory or Figure2:Aschematicdiagramdescribingknowledgegraph\\nduplicate facts, and aggregating or reconciling the information creationprocessofGraphRAG.\\nbasedonrules,probabilities,orsemanticsimilarity.\\nWeutilizedarobustmethodologyforcreatingKGtripletsfrom\\nunstructured text data, specifically focusing on corporate docu- 2.3.1 HybridRAG. FortheHybridRAGtechnique,weproposeto\\nmentssuchasearningscalltranscripts,adaptedfromRef.[18?]. integratetheaforementionedtwodistinctRAGtechniques:Vec-\\nThisprocessinvolvesseveralinterconnectedstages,eachdesigned torRAG and GraphRAG. This integration involves a systematic\\ntoextract,refine,andstructureinformationeffectively. combination of contextual information retrieved from both the\\nWeimplementatwo-tieredLLMchainforcontentrefinement traditionalvector-basedretrievalmechanismandtheKG-based\\nandinformationextraction.ThefirsttieremploysanLLMtogen- retrievalsystem,thelatterofwhichwasconstructedspecifically\\nerate an abstract representation of each document chunk. This forthisstudy.\\nrefinementprocessiscrucialasitdistillstheessentialinformation Theamalgamationofthesetwocontextsallowsustoleveragethe\\nwhilepreservingtheoriginalmeaningandkeyrelationshipsbe- strengthsofbothapproaches.TheVectorRAGcomponentprovides\\ntweenconceptsthatservesasamorefocusedinputforsubsequent abroad,similarity-basedretrievalofrelevantinformation,while\\nprocessing,enhancingtheoverallefficiencyandaccuracyofour theGraphRAGelementcontributesstructured,relationship-richSarmahetal.\\ncontextualdata.Thiscombinedcontextisthenutilizedasinputfora 2.4.2 AnswerRelevance:-. Theanswerrelevancemetricassesses\\nLLMtogeneratethefinalresponses.Detailsontheimplementation howwellthegeneratedansweraddressestheoriginalquestion,\\noftheHybridRAGwillbeprovidedinSection4.4. irrespectiveoffactualaccuracy.Thismetrichelpsidentifycasesof\\nincompleteanswersorresponsescontainingirrelevantinformation.\\nOurimplementationinvolvesthefollowingsteps:\\n2.4 EvaluationMetrics\\nQuestionGeneration:WeprompttheLLMtogeneratenpotential\\nToassesstheefficacyofthisintegratedapproach,weconducteda questionsbasedonthegivenanswer:\\ncomparativeanalysisamongthethreeapproachesinacontrolledex- \"Generateaquestionforthegivenanswer.answer:[answer]\".\\nperimentalsetup:VectorRAG,GraphRAGandHybridRAG.There- Then,weobtainembeddingsforallgeneratedquestionsandthe\\nsponsesgeneratedusingthecombinedVectorRAGandGraphRAG originalquestionusingOpenAI‚Äôstext-embedding-ada-002model2.\\ncontextswerejuxtaposedagainstthoseproducedindividuallyby Wethencalculatethecosinesimilaritybetweeneachgenerated\\nVectorRAGandGraphRAG.Thiscomparativeevaluationaimedto question‚Äôsembeddingandtheoriginalquestion‚Äôsembedding.\\ndiscernpotentialimprovementsinresponsequality,accuracy,and Finally,theanswerrelevancescore(AR)iscomputedastheaver-\\ncomprehensivenessthatmightarisefromthesynergisticintegra- agesimilarityacrossallgeneratedquestions:ùê¥ùëÖ= ùëõ1 (cid:205)(ùë†ùëñùëö(ùëû,ùëû ùëñ)),\\ntionofthesetwoRAGmethodologies. whereùë†ùëñùëö(ùëû,ùëû ùëñ)isthecosinesimilaritybetweentheembedding\\nToobjectivelyevaluatedifferentRAGapproaches(VectorRAG of the original questionùëû and the embeddings of each of theùëõ\\nand GraphRAGin their case), Ref. [22] utilized metrics such as generatedquestionsùëû ùëñ.\\ncomprehensiveness(i.e.,theamountofdetailstheanswerprovides\\nto cover all aspects and details of the question?); diversity (i.e., 2.4.3 ContextPrecision. Itisametricusedtoevaluatetherelevance\\ntherichnessoftheanswerinprovidingdifferentperspectivesand ofretrievedcontextchunksinrelationtoaspecifiedgroundtruth\\ninsightsonthequestion);empowerment(i.e.,thehelpfulnessofthe\\nforagivenquestion3.Itcalculatestheproportionofrelevantitems\\nanswertothereaderunderstandandmakeinformedjudgements thatappearinthetopranksofthecontext.Theformulaforcontext\\naboutthetopic);and,directness(i.e.,clearnessoftheanswerin precisionatKisthesumoftheproductsofprecisionateachrank\\naddressingthequestion).here,theLLMwasprovidedtuplesof kandabinaryrelevanceindicatorv_k,dividedbythetotalnumber\\nquestion,targetmetric,andapairofanswers,andwasaskedto ofrelevantitemsinthetopKresults.Precisionateachrankkis\\nassesswhichanswerwasbetteraccordingtothemetricandwhy. determinedbytheratiooftruepositivesatktothesumoftrue\\nThesemetricsthoughcomparethefinalgeneratedanswers,do positivesandfalsepositivesatk.Thismetrichelpsinassessing\\nnotnecessarilydirectlyevaluatetheretrievalandgenerationparts howwellthecontextsupportsthegroundtruth,aimingforhigher\\nseparately. Instead, here we implement a comprehensive set of scoreswhichindicatebetterprecision.\\nevaluationmetricswhicharedesignedtocapturedifferentaspects\\n2.4.4 ContextRecall. Itisametricusedtoevaluatehowwellthe\\nofagivenRAGsystem‚Äôsoutputquality,focusingonfaithfulness,\\nretrievedcontextalignswiththegroundtruthanswer,whichis\\nanswerrelevance,andcontextrelevance[30].Eachmetricprovides consideredthedefinitivecorrectresponse4.Itisquantifiedbycom-\\nuniqueinsightsintothesystem‚Äôscapabilitiesandlimitations.\\nparingeachsentenceinthegroundtruthanswertoseeifitcan\\nbetracedbacktotheretrievedcontext.Theformulaforcontext\\n2.4.1 Faithfulness. Faithfulnessisacrucialmetricthatmeasures recallistheratioofthenumberofgroundtruthsentencesthatcan\\ntheextenttowhichthegeneratedanswercanbeinferredfromthe beattributedtothecontexttothetotalnumberofsentencesin\\nprovidedcontext.Ourimplementationofthefaithfulnessmetric thegroundtruth.Highervalues,rangingfrom0to1,indicatebet-\\ninvolvesatwo-stepprocess: teralignmentandthusbettercontextrecall.Thismetriciscrucial\\nStatementExtraction:-WeuseanLLMtodecomposethegener- forassessingtheeffectivenessofinformationretrievalsystemsin\\natedanswerintoasetofconcisestatements.Thisstepiscrucial providingrelevantcontext.\\nforbreakingdowncomplexanswersintomoremanageableand\\nverifiableunits.Thepromptusedforthisstepis: 3 DATADESCRIPTION\\n\"Givenaquestionandanswer,createoneormorestatements\\nAlthoughtheredoexistsomepublicfinancialdatasets,noneofthem\\nfromeachsentenceinthegivenanswer.question:[question]an-\\nweresuitableforthepresentexperiments:e.g.,FinQA[31],TAT-\\nswer:[answer]\".\\nQA[32],FIQA[33],FinanceBench[34],etc.datasetsarelimitedto\\nStatementVerification:-Foreachextractedstatement,weemploy\\nspecificusecasessuchasbenchmarkingLLMs‚Äôabilitiestoperform\\ntheLLMtodetermineifitcanbeinferredfromthegivencontext.\\ncomplexnumericalreasoningorsentimentanalysis.Ontheother\\nThisverificationprocessusesthefollowingprompt:\\nhand, FinTextQA [35] dataset was not publicly available at the\\n\"Considerthegivencontextandfollowingstatements,thende-\\ntime of writing the present work. In addition, in most of these\\nterminewhethertheyaresupportedbytheinformationpresentin\\ndatasets,accesstotheactualdocumentsfromwhichtheground-\\nthecontext.Provideabriefexplanationforeachstatementbefore\\ntruthQ&Aswerecreatedisnotavailable,makingitimpossibleto\\narrivingattheverdict(Yes/No).Provideafinalverdictforeach\\nusethemforourRAGtechniquesevaluationpurposes.Hence,we\\nstatementinorderattheendinthegivenformat.Donotdeviate\\nresortedtoadatasetofourownthoughthroughpubliclyavailable\\nfromthespecifiedformat.statement:[statement1]...statement:\\n[statementn]\".\\nThefaithfulnessscore(ùêπ)isùêπ =|ùëâ|/|ùëÜ|,where|ùëâ|isthenumber 2 3h ht tt tp ps s: :/ // /p dola ct sf .o rr am ga.o s.p ioe /n ea ni. /c so tam b/ ld e/o cc os n/g cu epid te s/s m/e em trb ie csd /d ci on ng ts e/ xe tm _pb re ed cd ii sn iog n- .m hto mde lls\\nofsupportedstatementsand|ùëÜ|isthetotalnumberofstatements. 4https://docs.ragas.io/en/stable/concepts/metrics/context_recall.htmlHybridRAG:IntegratingKnowledgeGraphsandVectorRetrievalAugmentedGenerationforEfficientInformationExtraction\\ndocumentsbutsuchthatwefinallyhaveaccesstoboththeactual questionsposedduringtheearningscallsfromallthedocuments,\\nfinancial documents and the ground-truth Q&As. Datasets like andgatheredtheexactresponsescorrespondingtothesequestions.\\nFinanceBench5providesquestion-context-answertripletsbutthey Thesequestionsconstitutethespecificqueriesarticulatedbyfinan-\\narenotusefulasherewearecomparingVectorRAG,GraphRAG cialanalyststothemanagementduringthesecalls.\\nandHybridRAGandtheydonotprovidethecontextgenerated\\nfromaKG.Arecentpaper[22]hasnotmadetheKGandtriplets 4 IMPLEMENTATIONDETAILS\\nconstructedbytheiralgorithmpublictothebestofourknowledge InthisSection,weprovidedetailsofimplementationofthepro-\\neither. posedmethodology.\\nIn short, there is no publicly available benchmark dataset to\\ncompareVectorRAGandGraphRAGeitherforfinancialorgeneral 4.1 KnowledgeGraphConstruction\\ndomainstothebestofourknowledge.Hence,wehadtorelyon\\nTheinitialphaseofourapproachcentersondocumentpreprocess-\\nourowndatasetconstructedasexplainedbelow.\\ning.WeutilizethePyPDFLoader6toimportPDFdocuments,which\\nWeusedtranscriptsfromearningscallsofNifty50constituents\\naresubsequentlysegmentedintomanageablechunksusingthe\\nforouranalysis.TheNIFTY50ispopularindexintheIndianstock\\nRecursiveCharacterTextSplitter.Thischunkingstrategyemploysa\\nmarketthatrepresentstheweightedaverageof50ofthelargest\\nsizeof2024characterswithanoverlapof204characters,ensuring\\nIndian companies listed onthe National Stock Exchange (NSE).\\ncomprehensivecoveragewhilemaintainingcontextacrosssegment\\nThedatasetoftheearningcalldocumentsofNifty50companiesis\\nboundaries.\\nwidelyrecognizedintheinvestmentrealmandisesteemedasan\\nFollowingthepreprocessingstage,weimplementthetwo-tiered\\nauthoritativeandextensivecollectionofearningscalltranscripts.\\nlanguagemodelchainforcontentrefinementandinformationex-\\nInourinvestigation,wefocusondataspanningthequarterending\\ntraction.Itisnotpossibletoincludetheexactpromptheredueto\\ninJune,2023i.e.theearningsreportsforQ1ofthefinancialyear\\nthelimitedspace,butabaselinepromptcanbefoundinRef.[18].\\n2024(AfinancialyearinIndiastartsonthe1stAprilandendsin\\n31stMarch,sothequarterfrom1stAprilto30thJuneisthefirst\\nEntityType Examples\\nquarterof2024fortheIndianmarket).\\nCompaniesandCorpo- Officialnames,abbreviations,infor-\\nOurdatasetencompasses50transcriptsforthisquarter,span-\\nrations malreferences\\nning over 50 companies within Nifty 50 universe from diverse\\nrangeofsectorsincludingInfrastructure,Healthcare,Consumer Financial Metrics and Revenue,profitmargins,EBITDA\\nDurables,Banking,Automobile,FinancialServices,Energy-Oil Indicators\\n& Gas, Telecommunication, Consumer Goods, Pharmaceuticals, Corporate Executives CEOs,CFOs,boardmembers\\nEnergy-Coal,Materials,InformationTechnology,Construction, andKeyPersonnel\\nDiversified,Metals, Energy- Powerand Chemicalsproviding a ProductsandServices Tangibleproductsandintangibleser-\\nsubstantialanddiversefoundationforourstudy. vices\\nWestartthedatacollectionprocessfocusedonacquiringearn- GeographicalLocations Headquarters, operational regions,\\ningsreportsfromcompanywebsiteswithintheNifty50universeby markets\\ndevelopinganddeployingacustomwebscrapingtooltonavigate CorporateEvents Mergers, acquisitions, product\\nthroughthewebsitesofeachcompanywithintheNifty50index, launches,earningscalls\\nsystematicallyretrievingthepertinentearningsreportsforQ1of Legal and Regulatory Legalcases,regulatorycompliance\\nthefinancialyear2024.Byutilizingthiswebscrapingapproach, Information\\nweaimedtocompileacomprehensivedatasetencompassingthe Table2:Entitiesextractedfromearningscalltranscripts\\nearningsreportsoftheconstituentcompanies.\\nTable1summarizesbasicstatisticsofthedocumentswewillbe\\nexperimentingwithintheremainderofthiswork.\\nTable2summarizesdetailsonentitiesextractedfromtheearn-\\ningcallstranscriptsusingourpromptbasedmethod.Concurrently,\\nNumberofcompanies/documents 50\\nLLMidentifiesrelationshipsbetweentheseentitiesusingacurated\\nAveragenumberofpages 27\\nsetofverbs,capturingthenuancedinteractionswithinthecor-\\nAveragenumberofquestions 16\\nporatenarrative.Akeyimprovementinourmethodologyisthe\\nAveragenumberoftokens 60,000\\nenhancedpromptengineeringtogeneratethestructuredoutput\\nTable1:SummaryStatisticsforthecalltranscriptdocuments\\nformatforknowledgetriplets.Eachtripletisrepresentedasanested\\nusedinthepresentwork.\\nlist[‚Äôh‚Äô,‚Äôtype‚Äô,‚Äôr‚Äô,‚Äôo‚Äô,‚Äôtype‚Äô,‚Äômetadata‚Äô],where‚Äôh‚Äôand‚Äôo‚Äôdenote\\nThesecalltranscriptsdocumentsconsistofquestionsandan- theheadandobjectentitiesrespectively,‚Äôtype‚Äôspecifiestheentity\\nswersbetweenfinancialanalystsandthecompanyrepresentatives category,‚Äôr‚Äôrepresentstherelationship,and‚Äômetadata‚Äôencapsu-\\nfortherespectivecompanies,hence,therealreadyexistcertain latesadditionalcontextualinformation.Thisformatallowsfora\\nQ&Apairswithinthesedocumentsalongwithadditionaltext.We rich,multidimensionalrepresentationofinformation,facilitating\\nexaminedtheearningsreportswithintheNifty50universe,system- morenuanceddownstreamanalysis.\\naticallycuratedacomprehensivearrayofrandomlyselected400\\n6https://python.langchain.com/v0.1/docs/modules/data_connection/document_\\n5https://huggingface.co/datasets/PatronusAI/financebench-test loaders/pdf/Sarmahetal.\\nOurprocessincorporatesseveraladvancedfeaturestoenhance TheQ&ApipelineisconstructedusingtheLangChainframe-\\nthequalityandutilityoftheextractedtriplets.Entitydisambigua- work8.Thebeginswithacontextretrievalstep,wherewequerythe\\ntiontechniquesareemployedtoconsolidatedifferentreferencesto Pineconevectorstoretoobtainthemostrelevantdocumentchunks\\nthesameentity,improvingconsistencyacrosstheKG.Wealsopri- foragivenquestion.Thisretrievalprocessisfine-tunedwithspe-\\noritizeconcisenessinentityrepresentation,aimingfordescriptions cificfiltersforquarters,years,andcompanynames,ensuringthat\\noflessthanfourwordswherepossible,whichaidsinmaintaining theretrievedinformationisbothrelevantandcurrent.\\nacleanandnavigablegraphstructure. Followingretrieval,weimplementacontextformattingstepthat\\nTheextractionpipelineisappliediterativelytoeachdocument consolidatestheretrieveddocumentchunksintoacoherentcontext\\nchunk,withresultsaggregatedtoformacomprehensiveknowledge string.Thisformattedcontextservesastheinformationalbasisfor\\ngraphrepresentationoftheentiredocument,allowingforscalable thelanguagemodel‚Äôsresponsegeneration.Wehavedevelopeda\\nprocessing of large documents while maintaining local context sophisticatedprompttemplate,thatinstructsthelanguagemodel\\nwithineachchunk.Wehaveaddedexplicitinstructiononobtaining tofunctionasanexpertQ&Asystem,emphasizingtheimportance\\nmetadatafollowingRef.[8]forbothVectorRAGandGraphRAG. ofutilizingonlytheprovidedcontextinformationandavoiding\\nFinally,weimplementadatapersistencestrategy,converting directreferencestothecontextinthegeneratedresponses.\\ntheextractedtripletsfromtheirinitialstringformattoPythondata Forthecorelanguageprocessingtask,weintegrateOpenAI‚Äôs\\nstructuresandstoringtheminapicklefile.Thisfacilitateseasy GPT-3.5-turbomodelwhichprocessestheformattedcontextand\\nretrievalandfurthermanipulationoftheknowledgegraphdatain querytogeneratenaturallanguageresponsesthatareinformative,\\nsubsequentanalysisstages. coherent,andcontextuallyappropriate.\\nOurmethodologyrepresentsasignificantadvancementinauto- To evaluate the performance of our system, we developed a\\nmatedknowledgeextractionfromcorporatedocuments.Bycom- comprehensiveframeworkthatincludesthepreparationofacus-\\nbiningadvancedNLPtechniqueswithastructuredapproachto tomdatasetofquestion-answerpairsspecifictoeachcompany‚Äôs\\ninformationrepresentation,wecreatearich,queryableknowledge earningscall.Oursystemprocesseseachquestioninthisdataset,\\nbasethatcapturesthecomplexrelationshipsandkeyinformation generatinganswersbasedontheretrievedcontext.Theevaluation\\npresentincorporatenarratives.Thisapproachopensupnewpos- results,includingtheoriginalquestion,generatedanswer,retrieved\\nsibilities for financial analysis and automated reasoning in the contexts,andgroundtruth,arecompiledintostructuredformats\\nbusinessdomainthatwillbeexploredfurtherinthefuture. (CSVandJSON)tofacilitatefurtheranalysis.Theoutputsgenerated\\nbyoursystemarestoredinbothCSVandJSONformats,enabling\\n4.2 VectorRAG easyintegrationwithvariousanalysistoolsanddashboards.This\\nOurmethodologybuildsupontheconceptofRAG[9]whichal- approach facilitates both quantitative performance metrics and\\nlowsforthecreationofasystemthatcanprovidecontext-aware, qualitativeassessmentofthesystem‚Äôsresponses,providingacom-\\naccurateresponsestoqueriesaboutcompanyfinancialinforma- prehensiveviewofitseffectiveness.\\ntion,leveragingboththepoweroflargelanguagemodelsandthe Byparameterizingcompanynames,quarters,andyears,wecan\\nefficiencyofsemanticsearch. easilyadaptthesystemtodifferentdatasetsandtimeperiods.This\\nAtthecoreofoursystemisaPineconevectordatabase7,which design choice allows for seamless integration of new data and\\nservesasthefoundationforourinformationretrievalprocess.We expansiontocovermultiplecompaniesandearningscalls.\\nemployOpenAI‚Äôstext-embedding-ada-002modeltotransformtex-\\n4.3 GraphRAG\\ntualdatafromearningscalltranscriptsintohigh-dimensionalvec-\\ntorrepresentations.Thisvectorizationprocessenablessemantic ForGraphRAG,wedevelopedanQ&Asystemspecificallydesigned\\nsimilaritysearches,significantlyenhancingtherelevanceandac- for corporate earnings call transcripts. Our implementation of\\ncuracyofretrievedinformation.Table3providessummaryofthe GraphRAGleveragesseveralkeycomponentsandtechniques:\\nconfigurationofthesetupinuseforourexperiments.\\nLLM GPT-3.5-Turbo\\nLLM GPT-3.5-Turbo\\nLLMTemperature 0\\nLLMTemperature 0\\nFramework LangChain\\nEmbeddingModel text-embedding-ada-002\\nKGManipulation Networkx\\nFramework LangChain\\nChunkSize 1024\\nVectorDatabase Pinecone\\nChunkOverlap 0\\nChunkSize 1024\\nNumberofTriplets 13950\\nChunkOverlap 0\\nNumberofnodes 11405\\nMaximumOutputTokens 1024\\nNumberofedges 13883\\nChunksforSimilarityAlgorithm 20\\nDFSDepth 1\\nNumberofContextRetrieved 4\\nTable4:GraphRAGConfiguration\\nTable3:VectorRAGConfiguration\\n7https://www.pinecone.io/ 8https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chainHybridRAG:IntegratingKnowledgeGraphsandVectorRetrievalAugmentedGenerationforEfficientInformationExtraction\\nKnowledge Graph Construction:- We begin by constructing presentworkwherewehavemodifiedthemabittomakemore\\naKGfromasetofknowledgetripletsextractedfromcorporate precisecomparisons.\\ndocumentsusingthepromptengineeringbasedmethodologyas Theresultsofourcomparativeanalysisrevealnotabledifferences\\ndescribedinSection4.1.Thesetriplets,storedinapicklefile,repre- inperformanceamongVectorRAG,GraphRAG,andHybridRAG\\nsentstructuredinformationintheformofsubject-predicate-object approaches as summarized in Table 5. In terms of Faithfulness,\\nrelationships. We use the NetworkxEntityGraph class from the GraphRAGandHybridRAGdemonstratedsuperiorperformance,\\nLangChainlibrarytocreateandmanagethisgraphstructure.Each bothachievingascoreof0.96,whileVectorRAGtrailedslightlywith\\ntripleisaddedtothegraph,whichencapsulatestheheadentity, ascoreof0.94.Answerrelevancyscoresvariedacrossthemeth-\\nrelation,tailentity,andadditionalmetadata. ods,withHybridRAGoutperformingtheothersat0.96,followed\\nWeimplementtheQ&AfunctionalityusingtheGraphQAChain byVectorRAGat0.91,andGraphRAGat0.89.Contextprecision\\nclassfromLangChain.ThischaincombinestheKGwithanLLM washighestforGraphRAGat0.96,significantlysurpassingVec-\\n(inourcase,OpenAI‚ÄôsGPT-3.5-turbo)togenerateresponses.The torRAG(0.84)andHybridRAG(0.79).However,incontextrecall,\\nGraphQAChaintraversestheKGtofindrelevantinformationand bothVectorRAGandHybridRAGachievedperfectscoresof1,while\\nusesthelanguagemodeltoformulatecoherentanswersbasedon GraphRAGlaggedbehindat0.85.\\nthe retrieved context. A summary of configuration of our LLM Overall,theseresultssuggestthatGraphRAGoffersimprove-\\nmodelsandotherlibrariesusedforGraphRAGisshowninTable4. mentsoverVectorRAG,particularlyinfaithfulnessandcontextpre-\\nInKGastheinformationisstoredintheformofentitiesand cision.Furthermore,HybridRAGemergesasthemostbalancedand\\nrelationshipsandtherecanbemultiplerelationsemanatingfrom effectiveapproach,outperformingbothVectorRAGandGraphRAG\\nonesingleentity,inthisexperiment,toextractrelevantinformation inkeymetricssuchasfaithfulnessandanswerrelevancy,while\\nfromtheKG,weemployadepth-firstsearchstrategyconstrained maintaininghighcontextrecall.\\nbyadepthofonefromthespecifiedentity. TherelativelylowercontextprecisionobservedforHybridRAG\\nTo prepare for assessing the performance of our GraphRAG (0.79)canbeattributedtoitsuniqueapproachofcombiningcon-\\nsystem,wefollowthebelowsteps:DatasetPreparation:-Weuse textsfrombothVectorRAGandGraphRAGmethods.Whilethis\\napre-generatedCSVfilecontainingquestion-answerpairsspecific integrationallowsformorecomprehensiveinformationretrieval,\\ntotheearningscalltranscript. italsointroducesadditionalcontentthatmaynotalignprecisely\\nIterativeProcessing:-Foreachquestioninthedataset,werun withthegroundtruth,thusaffectingthecontextprecisionmet-\\ntheGraphQAChaintogenerateananswer. ric.Despitethistrade-off,HybridRAG‚Äôssuperiorperformancein\\nResultCompilation:-Wecompiletheresults,includingtheorigi- faithfulness,answerrelevancy,andcontextrecallunderscoresits\\nnalquestions,generatedanswers,retrievedcontexts,andground effectiveness. When considering the overall evaluation metrics,\\ntruthanswers,intoastructuredformat. HybridRAGemergesasthemostpromisingapproach,balancing\\nFinally,theevaluationresultsaresavedinbothCSVandJSON high-qualityanswerswithcomprehensivecontextretrieval.\\nformatsforfurtheranalysisandcomparison.Wethenfedthese OverallGraphRAGperformsbetterinextractivequestionscom-\\noutputsintoourRAGevaluationpipeline.ForeachQ&Apairin paredtoVectorRAG.AndVectorRAGdoesbetterinabstractive\\nour dataset, we compute all three metrics: faithfulness, answer questions where information is not explicitly mentioned in the\\nrelevance,contextprecisionandcontextrecall. raw data. And also GraphRAG sometimes fails to answer ques-\\ntionscorrectlywheneverthereisnoentityexplicitlymentionedin\\n4.4 HybridRAG thequestion.SoHybridRAGdoesagoodjoboverall,aswhenever\\nVectorRAGfailstofetchcorrectcontextinextractivequestions\\nFortheproposedHybridRAGtechnique,uponobtainingallthe\\nitfallsbacktoGraphRAGtogeneratetheanswer.Andwhenever\\ncontextualinformationfromVectorRAGandGraphRAG,wecon-\\nGraphRAGfailstofetchcorrectcontextinabstractivequestionsit\\ncatenatethesecontextstoformaunifiedcontextutilizingboth\\nfallsbacktoVectorRAGtogeneratetheanswer.\\ntechniques.Thiscombinedcontextisthenfedintotheanswergen-\\neratormodeltoproducearesponse.Thecontextusedforresponse\\nVectorRAG GraphRAG HybridRAG\\ngenerationisrelativelylarger,whichaffectstheprecisionofthe\\nF 0.94 0.96 0.96\\ngeneratedresponse.ThecontextfromVectorRAGisappendedfirst,\\nAR 0.91 0.89 0.96\\nfollowedbythecontextfromGraphRAG.Consequently,thepreci-\\nsionofthegeneratedanswerdependsonthesourcecontext.Ifthe CP 0.84 0.96 0.79\\nanswerisgeneratedfromtheGraphRAGcontext,itwillhavelower CR 1 0.85 1\\nprecision,astheGraphRAGcontextisaddedlastinthesequenceof Table5:PerformanceMetricsforDifferentRAGPipelines.\\ncontextsprovidedtotheanswergeneratormodel,andviceversa. Here, F, AR, CP and CR refer to Faithfulness, Answer\\nRelevence,ContextPrecisionandContextRecall.\\n5 RESULTS\\nWeevaluateboththeretrievalandgenerationpartsofRAGfor\\nthethreedifferentRAGpipelines.EvaluatingtheRAGoutputsis\\n6 CONCLUSIONANDFUTUREDIRECTIONS\\nalsoanactiveareaofresearchthereisnostandardtoolwhichis\\nuniversallyacceptedasofyet,thoughweuseacurrentlypopular Amongthecurrentapproachestomitigateissuesregardinginfor-\\nframeworkRAGAS[30]toevaluatethethreeRAGpipelinesinthe mationextractionfromexternaldocumentsthatwerenotpartofSarmahetal.\\ntrainingdatafortheLLM,RetrievalAugmentedGeneration(RAG) [3] YiYang,MarkChristopherSiyUy,andAllenHuang. Finbert:Apretrained\\ntechniqueshaveemergedasthemostpopularonesthataimto languagemodelforfinancialcommunications.arXivpreprintarXiv:2006.08097,\\n2020.\\nimprovetheperformanceofLLMsbyincorporatingrelevantre-\\n[4] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Se-\\ntrievalmechanisms.RAGmethodsenhancetheLLMs‚Äôcapabilities bastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon\\nbyretrievingpertinentdocumentsortexttoprovideadditional Mann. Bloomberggpt:Alargelanguagemodelforfinance. arXivpreprint\\narXiv:2303.17564,2023.\\ncontextduringthegenerationprocess.However,theseapproaches [5] YuqiNie,YaxuanKong,XiaowenDong,JohnMMulvey,HVincentPoor,Qing-\\nencountersignificantlimitationswhenappliedtothespecialized songWen,andStefanZohren.Asurveyoflargelanguagemodelsforfinancialap-\\nplications:Progress,prospectsandchallenges.arXivpreprintarXiv:2406.11903,\\nandintricatedomainoffinancialdocuments.Furthermore,thequal-\\n2024.\\nityoftheretrievedcontextfromavastandheterogeneouscorpus [6] HuaqinZhao,ZhengliangLiu,ZihaoWu,YiweiLi,TianzeYang,PengShu,\\ncanbeinconsistent,leadingtoinaccuraciesandincompleteanalyses. ShaochenXu,HaixingDai,LinZhao,GengchenMai,NinghaoLiu,andTianming\\nLiu.Revolutionizingfinancewithllms:Anoverviewofapplicationsandinsights,\\nThesechallengeshighlighttheneedformoresophisticatedmethods\\n2024.\\nthatcaneffectivelyintegrateandprocessthedetailedanddomain- [7] ChenLing,XujiangZhao,JiayingLu,ChengyuanDeng,CanZheng,Junxiang\\nspecificinformationfoundinfinancialdocuments,ensuringmore Wang,TanmoyChowdhury,YunLi,HejieCui,XuchaoZhang,etal.Domainspe-\\ncializationasthekeytomakelargelanguagemodelsdisruptive:Acomprehensive\\nreliableandaccurateoutputsforinformeddecision-making. survey.arXivpreprintarXiv:2305.18703,2023.\\nInthepresentwork,wehaveintroducedanovelapproachthat [8] BhaskarjitSarmah,DhagashMehta,StefanoPasquali,andTianjieZhu.Towards\\nreducinghallucinationinextractinginformationfromfinancialreportsusing\\nsignificantlyadvancesthefieldofinformationextractionfromfinan-\\nlargelanguagemodels. InProceedingsoftheThirdInternationalConference\\ncialdocumentsthroughthedevelopmentofahybridRAGsystem. onAI-MLSystems,pages1‚Äì5,2023.\\nThis system, called HybridRAG, which integrates the strengths [9] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\\nKarpukhin,NamanGoyal,HeinrichK√ºttler,MikeLewis,Wen-tauYih,Tim\\nofbothKnowledgeGraphs(KGs)andadvancedlanguagemodels,\\nRockt√§schel,etal.Retrieval-augmentedgenerationforknowledge-intensivenlp\\nrepresentsaleapforwardinourabilitytoextractandinterpret tasks.AdvancesinNeuralInformationProcessingSystems,33:9459‚Äì9474,2020.\\ncomplexinformationfromunstructuredfinancialtexts.Thehybrid [10] GautierIzacardandEdouardGrave.Leveragingpassageretrievalwithgenerative\\nmodelsforopendomainquestionanswering.arXivpreprintarXiv:2007.01282,\\nRAGsystem,bycombiningtraditionalvector-basedRAGandKG- 2021.\\nbasedRAG,hasshownsuperiorperformanceintermsofretrieval [11] KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMing-WeiChang.\\nRealm: Retrieval-augmented language model pre-training. arXiv preprint\\naccuracyandanswergeneration,markingapivotalsteptowards\\narXiv:2002.08909,2020.\\nmoreeffectivefinancialanalysistools. [12] AntonioJoseJimenoYepesetal.Financialreportchunkingforeffectiveretrieval\\nThroughacomparativeanalysisusingobjeciveevaluationmet- augmentedgeneration.arXivpreprintarXiv:2402.05131,2024.\\n[13] SuperAcc.Retrieval-augmentedgenerationonfinancialstatements.SuperAcc\\nrics,wehavehighlightedthedistinctperformanceadvantagesof\\nInsights,2024.\\ntheHybridRAGapproachoveritsvector-basedandKG-basedcoun- [14] ShaoxiongJi,ShiruiPan,ErikCambria,PekkaMarttinen,andSYuPhilip. A\\nterparts. The HybridRAG system excels in faithfulness, answer surveyonknowledgegraphs:Representation,acquisition,andapplications.IEEE\\ntransactionsonneuralnetworksandlearningsystems,33(2):494‚Äì514,2021.\\nrelevancy,andcontextrecall,demonstratingthebenefitsofinte- [15] HeikoPaulheim. Knowledgegraphs:Stateoftheartandfuturedirections.\\ngratingcontextsfrombothVectorRAGandGraphRAGmethods, SemanticWebJournal,10(4):1‚Äì20,2017.\\n[16] AidanHogan,EvaBlomqvist,MichaelCochez,ClaudiaD‚ÄôAmato,GerarddeMelo,\\ndespitepotentialtrade-offsincontextprecision.\\nClaudioGuti√©rrez,andMaria...Maleshkova.Knowledgegraphs.arXivpreprint\\nTheimplicationsofthisresearchextendbeyondtheimmedi- arXiv:2003.02320,2021.\\naterealmoffinancialanalysis.Bydevelopingasystemcapable [17] LisaEhrlingerandWolframW√∂√ü.Towardsadefinitionofknowledgegraphs.In\\nSEMANTiCS(Posters,Demos,SuCCESS),pages1‚Äì4,2016.\\nofunderstandingandrespondingtonuancedqueriesaboutcom-\\n[18] XiaohuiVictorLiandFrancescoSannaPassino.Findkg:Dynamicknowledge\\nplexfinancialdocuments,wepavethewayformoresophisticated graphswithlargelanguagemodelsfordetectingglobaltrendsinfinancialmar-\\nAI-assistedfinancialdecision-makingtoolsthatcouldpotentially kets.arXivpreprintarXiv:2407.10909,2024.\\n[19] ShouryaDe,AnimaAggarwal,andAnnaCinziaSquicciarini.Financialknowl-\\ndemocratizeaccesstofinancialinsights,allowingabroaderrangeof edgegraphs:Anovelapproachtoempowerdata-drivenfinancialapplications.In\\nstakeholderstoengagewithandunderstandfinancialinformation. IEEEInternationalConferenceonBigData(BigData),pages1311‚Äì1320,2018.\\n[20] MarinaPetrovaandBirgitReinwald.Knowledgegraphsinfinance:Applications\\nFuturedirectionsforthisresearchincludeexpandingthesystem\\nandopportunities.JournalofFinancialDataScience,2(2):10‚Äì19,2020.\\ntohandlemulti-modalinputs,incorporatingnumericaldataanalysis [21] WeiLiu,JunZhang,andLiPan.Utilizingknowledgegraphsforfinancialdata\\ncapabilities,anddevelopingmoresophisticatedevaluationmetrics integrationandanalysis.DataScienceJournal,18(1):1‚Äì15,2019.\\n[22] DarrenEdge,HaTrinh,NewmanCheng,JoshuaBradley,AlexChao,Apurva\\nthatcapturethenuancesoffinanciallanguageandtheaccuracy\\nMody,StevenTruitt,andJonathanLarson. Fromlocaltoglobal:Agraphrag\\nofnumericalinformationintheresponses.Additionally,exploring approachtoquery-focusedsummarization. arXivpreprintarXiv:2404.16130,\\ntheintegrationofthissystemwithreal-timefinancialdatastreams 2024.\\n[23] XuchenYao,YananSun,ZhenHuang,andDongLi.Retrieval-augmentedgener-\\ncouldfurtherenhanceitsutilityindynamicfinancialenvironments. ationforknowledge-intensivenlptasks.arXivpreprintarXiv:2101.07554,2021.\\n[24] WeizhiZhao,HaoyuChen,KaiLiu,andJunZhao. Graph-basedretrieval-\\n7 ACKNOWLEDGEMENT augmentedgenerationforopen-domainquestionanswering. InProceedings\\noftheAAAIConferenceonArtificialIntelligence,volume36,pages3098‚Äì3106,\\nTheviewsexpressedherearethoseoftheauthorsaloneandnotof 2022.\\n[25] BillYuchenLin,YuanheLiu,MingShen,andXiangRen. Kgpt:Knowledge-\\nBlackRock,IncorNVIDIA.WearegratefultoEmmaLindforher\\ngroundedpre-trainingfordata-to-textgeneration.InFindingsoftheAssociation\\ninvaluablesupportforthiscollaboration. forComputationalLinguistics:EMNLP2020,pages710‚Äì724,2020.\\n[26] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,Ji-\\nREFERENCES aweiSun,andHaofenWang.Retrieval-augmentedgenerationforlargelanguage\\nmodels:Asurvey.arXivpreprintarXiv:2312.10997,2023.\\n[1] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.Efficientestimation [27] TylerProcko.Graphretrieval-augmentedgenerationforlargelanguagemodels:\\nofwordrepresentationsinvectorspace.2013. Asurvey.AvailableatSSRN,2024.\\n[2] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones, [28] LingfengZhong,JiaWu,QianLi,HaoPeng,andXindongWu.Acomprehensive\\nAidanNGomez,≈ÅukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed. surveyonautomaticknowledgegraphconstruction.ACMComputingSurveys,\\nAdvancesinneuralinformationprocessingsystems,30,2017. 56(4):1‚Äì62,2023.HybridRAG:IntegratingKnowledgeGraphsandVectorRetrievalAugmentedGenerationforEfficientInformationExtraction\\n[29] IshaniMondal,YufangHou,andCharlesJochim. End-to-endnlpknowledge webconference2018,pages1941‚Äì1942,2018.\\ngraphconstruction.arXivpreprintarXiv:2106.01167,2021. [33] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang,\\n[30] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ra- JianchengLv,FuliFeng,andTat-SengChua. Tat-qa:Aquestionanswering\\ngas:Automatedevaluationofretrievalaugmentedgeneration.arXivpreprint benchmarkonahybridoftabularandtextualcontentinfinance.arXivpreprint\\narXiv:2309.15217,2023. arXiv:2105.07624,2021.\\n[31] ZhiyuChen,WenhuChen,ChareseSmiley,SameenaShah,IanaBorova,Dylan [34] PranabIslam,AnandKannappan,DouweKiela,RebeccaQian,NinoScherrer,\\nLangdon,ReemaMoussa,MattBeane,Ting-HaoHuang,BryanRoutledge,etal. andBertieVidgen. Financebench:Anewbenchmarkforfinancialquestion\\nFinqa:Adatasetofnumericalreasoningoverfinancialdata. arXivpreprint answering.arXivpreprintarXiv:2311.11944,2023.\\narXiv:2109.00122,2021. [35] JianChen,PeilinZhou,YiningHua,YingxinLoh,KehuiChen,ZiyuanLi,Bing\\n[32] MacedoMaia,SiegfriedHandschuh,Andr√©Freitas,BrianDavis,RossMcDermott, Zhu,andJunweiLiang.Fintextqa:Adatasetforlong-formfinancialquestion\\nManelZarrouk,andAlexandraBalahur. Www‚Äô18openchallenge:financial answering.arXivpreprintarXiv:2405.09980,2024.\\nopinionminingandquestionanswering.InCompanionproceedingsofthethe'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "# # Load the PDF file\n",
    "# pdf_path = \"2408.04948v1.pdf\"\n",
    "# reader = PdfReader(pdf_path)\n",
    "\n",
    "# # Extract text from each page\n",
    "# text = \"\"\n",
    "# for page in reader.pages:\n",
    "#     text += page.extract_text()\n",
    "\n",
    "# # Print or save the extracted text\n",
    "# print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "text = extract_text('2408.04948v1.pdf')\n",
    "with open(\"output.txt\",\"w\",encoding='utf-8') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Apply the replacements\n",
    "text_modified = re.sub(r'-\\n', '', text)\n",
    "text_modified = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4 2 0 2\\n\\ng u A 9\\n\\n] L C . s c [\\n\\n1 v 8 4 9 4 0 . 8 0 4 2 : v i X r a\\n\\nHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\n\\nBhaskarjit Sarmah bhaskarjit.sarmah@blackrock.com BlackRock, Inc. Gurugram, India\\n\\nBenika Hall bhall@nvidia.com NVIDIA Santa Clara, CA, USA\\n\\nRohan Rao rohrao@nvidia.com NVIDIA Santa Clara, CA, USA\\n\\nSunil Patel supatel@nvidia.com NVIDIA Santa Clara, CA, USA\\n\\nStefano Pasquali stefano.pasquali@blackrock.com BlackRock, Inc. New York, NY, USA\\n\\nDhagash Mehta dhagash.mehta@blackrock.com BlackRock, Inc. New York, NY, USA\\n\\nABSTRACT Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.\\n\\n1 INTRODUCTION For the financial analyst, it is crucial to extract and analyze information from unstructured data sources like news articles, earnings reports, and other financial documents to have at least some chance to be on the better side of potential information asymmetry. These sources hold valuable insights that can impact investment decisions, market predictions, and overall sentiment. However, traditional data analysis methods struggle to effectively extract and utilize this information due to its unstructured nature. Large language models (LLMs) [1‚Äì4] have emerged as powerful tools for financial services and investment management. Their ability to process and understand vast amounts of textual data makes them invaluable for tasks such as sentiment analysis, market trend predictions, and automated report generation. Specifically, extracting information from annual reports and other financial documents can greatly enhance the efficiency and accuracy of financial analysts [5]. A robust information extraction system can help analysts quickly gather relevant data, identify market trends, and make informed decisions, leading to better investment strategies and risk management [6].\\n\\nAlthough LLMs have substantial potential in financial applications, there are notable challenges in using pre-trained models to extract information from financial documents outside their training data while also reducing hallucination [7, 8]. Financial documents typically contain domain-specific language, multiple data formats, and unique contextual relationships that general purpose-trained LLMs do not handle well. In addition, extracting consistent and coherent information from multiple financial documents can be challenging due to variations in terminology, format, and context across different textual sources. The specialized terminology and complex data formats in financial documents make it difficult for models to extract meaningful insights, in turn, causing inaccurate predictions, overlooked insights, and unreliable analysis, which ultimately hinder the ability to make well-informed decisions.\\n\\nCurrent approaches to mitigate these issues include various Retrieval-Augmented Generation (RAG) techniques [9], which aim to improve the performance of LLMs by incorporating relevant retrieval techniques. VectorRAG (the traditional RAG techniques that are based on vector databases) focuses on improving Natural Language Processing (NLP) tasks by retrieving relevant textual information to support the generation tasks. VectorRAG excels in situations where context from related textual documents is crucial for generating meaningful and coherent responses [9‚Äì11]. RAGbased methods ensure the LLMs generate relevant and coherent responses that are aligned with the original input query. However, for financial documents, these approaches have significant challenges as a standalone solution. For instance, traditional RAG systems often use paragraph-level chunking techniques, which assume the text in those documents are uniform in length. This approach neglects the hierarchical nature of financial statements and can result in the loss of critical contextual information for an accurate analysis[12, 13]. Due to the complexities in analyzing financial documents, the quality of the LLM retrieved-context from a vast and heterogeneous corpus can be inconsistent, leading to inaccuracies and incomplete analyses. These challenges demonstrate the need for more sophisticated methods that can effectively integrate and process the detailed and domain-specific information found in financial documents, ensuring more reliable and accurate results for informed decision-making.\\n\\nKnowledge graphs (KGs) [14] may provide a different point of view to looking at the financial documents where the documents\\n\\n            \\x0care viewed as a collection of triplets of entities and their relationships as depicted in the text of the documents. KGs have become a pivotal technology in data management and analysis, providing a structured way to represent knowledge through entities and relationships and have been widely adopted in various domains, including search engines, recommendation systems, and biomedical research [15‚Äì17]. The primary advantage of KGs lies in their ability to offer a structured representation, which facilitates efficient querying and reasoning. However, building and maintaining KGs and integrating data from different sources, such as documents, news articles, and other external sources, into a coherent knowledge graph poses significant challenges.\\n\\nThe financial services industry has recognized the potential of KGs in enhancing data integration of heterogeneous data sources, risk management, and predictive analytics [18? ‚Äì21]. Financial KGs integrate various financial data sources such as market data, financial reports, and news articles, creating a comprehensive view of financial entities and their relationships. This unified view improves the accuracy and comprehensiveness of financial analysis, facilitates risk management by identifying hidden relationships, and supports advanced predictive analytics for more accurate market predictions and investment decisions. However, handling large volumes of financial data and continuously updating the knowledge graph to reflect the dynamic nature of financial markets can be challenging and resource-intensive.\\n\\nGraphRAG (Graph-based Retrieval-Augmented Generation) [22‚Äì 27] is a novel approach that leverages knowledge graphs (KGs) to enhance the performance of NLP tasks such as Q&A systems. By integrating KGs with RAG techniques, GraphRAG enables more accurate and context-aware generation of responses based on the structured information extracted from financial documents. But GraphRAG generally underperforms in abstractive Q&A tasks or when there is not explicit entity mentioned in the question.\\n\\nIn the present work, we propose a combination of VectorRAG and GraphRAG, called HybridRAG, to retrieve the relevant information from external documents for a given query to the LLM that brings advantages of both the RAGs together to provide demonstrably more accurate answers to the queries.\\n\\n1.1 Prior Work and Our Contribution VectorRAG has been extensively investigated in the recent years and focuses on enhancing NLP tasks by retrieving relevant textual information to support generation processes [9‚Äì11, 26]. However, the effectiveness of the retrieval mechanism across multiple documents and longer contexts poses a significant challenge in extracting relevant responses. GraphRAG combines the capabilities of KGs with RAG to improve traditional NLP tasks [23‚Äì25]. Within our implementations of both VectorRAG GraphRAG techniques, we explicitly add information on the metadata of the documents that is also shown to improve the performance of VectorRAG [8].\\n\\nTo the best of our knowledge the present work is the first work that proposes a RAG approach that is hybrid of both VectorRAG and GraphRAG and demonstrates its potential of more effective analysis and utilization of financial documents by leveraging the combined strengths of VectorRAG and GraphRAG. We also utilize a novel ground-truth Q&A dataset extracted from publicly available\\n\\nSarmah et al.\\n\\nfinancial call transcripts of the companies included in the Nifty-50 index which is an Indian stock market index that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange1.\\n\\n2 METHODOLOGY In this Section, we provide details of the proposed methodology by first discussing details of VectorRAG, then methodologies of constructing KGs from given documents and our proposed methodology of GraphRAG and finally the HybridGraph technique.\\n\\n2.1 VectorRAG The traditional RAG [9] process begins with a query that is related to the information possessed within external document(s) that are not a part of the training dataset for the LLM. This query is used to search an external repository, such as a vector database or indexed corpus, to fetch relevant documents or passages containing useful information. These retrieved documents are then fed back into the LLM as additional context. Hence, in turn, for the given query, the language model generates a response based not only on its internal training data but also by incorporating the retrieved external information. This integration ensures that the generated content is grounded in more recent and verifiable data, improving the accuracy and contextual relevance of the responses. By combining the retrieval of external information with the generative capabilities of large language models, RAG enhances the overall quality and reliability of the generated text.\\n\\nIn traditional VectorRAG, the given external documents are divided into multiple chunks because of the limitation of context size of the language model. Those chunks are converted into embeddings using an embeddings model and then stored into a vector database. After that, the retrieval component performs a similarity search within the vector database to identify and rank the chunks most relevant to the query. The top-ranked chunks are retrieved and aggregated to provide context for the generative model.\\n\\nThen, the generative model takes this retrieved context along with the original query and synthesizes a response. Thus, it merges the real-time information from the retrieved chunks with its preexisting knowledge, ensuring that the response is both contextually relevant and detailed.\\n\\nThe schematic diagram in Figure 1 provides details on the part of RAG that generates vector database from given external documents in the traditional VectorRAG methodology where we also include explicit reference of metadata [8]. Section 4.2 will provide implementation details for our experiments.\\n\\n2.2 Knowledge Graph Construction A KG is a structured representation of real-world entities, their attributes, and their relations, usually stored in a graph database or a triplet store, i.e., a KG consists of nodes that represent entities and edges that represent relations, as well as labels and attributes for both. A graph triplet is a basic unit of information in a KG, consisting of a subject, a predicate, and an object.\\n\\nIn most methodologies to build a KG from given documents, three main steps are involved: knowledge extraction, knowledge\\n\\n1https://www.nseindia.com/products/content/equities/indices/nifty_50.htm\\n\\n\\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\n\\ntriplet extraction pipeline. The second tier of our LLM chain is dedicated to entity extraction and relationship identification.\\n\\nBoth the steps are executed using carefully performed prompt engineering on a pre-trained LLM. A detailed discussion on implementation of the methodology will be provided in Section 4.1\\n\\n2.3 GraphRAG KG based RAG [22], or GraphRAG, also begins with a query based on the user‚Äôs input same as VectorRAG. The main difference between VectorRAG and GraphRAG lies in the retrieval part. The query here is now used to search the KG to retrieve relevant nodes (entities) and edges (relationships) related to the query. A subgraph, which consists of these relevant nodes and edges, is extracted from the full KG to provide context. This subgraph is then integrated with the language model‚Äôs internal knowledge, by encoding the graph structure into embeddings that the model can interpret. The language model uses this combined context to generate responses that are informed by both the structured information from the KG and its pre-trained knowledge. Crucially, when responding to user queries about a particular company, we leveraged the metadata information to selectively filter and retrieve only those document segments pertinent to the queried company [8]. This integration helps ensure that the generated outputs are accurate, contextually relevant, and grounded in verifiable information.\\n\\nA schematic diagram of the retrieval methodology of GraphRAG is given in Figure 2. Here we first write a prompt to clean the data and then write another prompt in the second stage to create knowledge triplets along with metadata. It will be covered in more detail in section 4.1\\n\\nFigure 2: A schematic diagram describing knowledge graph creation process of GraphRAG.\\n\\n2.3.1 HybridRAG. For the HybridRAG technique, we propose to integrate the aforementioned two distinct RAG techniques: VectorRAG and GraphRAG. This integration involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system, the latter of which was constructed specifically for this study.\\n\\nThe amalgamation of these two contexts allows us to leverage the strengths of both approaches. The VectorRAG component provides a broad, similarity-based retrieval of relevant information, while the GraphRAG element contributes structured, relationship-rich\\n\\nFigure 1: A schematic diagram describing the vector database creation of a RAG application.\\n\\nimprovement, and knowledge adaptation [28]. Here, we do not use knowledge adaptation and treat the KGs as static graphs. Knowledge Extraction:- This step aims to extract structured information from unstructured or semi-structured data, such as text, databases, and existing ontologies. The main tasks in this step are entity recognition, relationship extraction, and co-reference resolution. Entity recognition and relationship extraction techniques use typical NLP tasks to identify entities and their relationships from textual sources [29]. Coreference resolution identifies and connects different references of the same entity, keeping coherence within the knowledge graph. For example, if the text refers to a company as both \"the company\" and \"it\", coreference resolution can link these mentions to the same entity node in the graph. Knowledge Improvement:- This step aims to enhance the quality and completeness of the KG by removing redundancies and addressing gaps in the extracted information. The primary tasks in this step are KG completion and fusion. KG completion technique infers missing entities and relationships within the graph using methods such as link prediction and entity resolution. Link prediction predicts the existence and type of a relation between two entities based on the graph structure and features, while entity resolution matches and merges different representations of the same entity from different sources.\\n\\nKnowledge fusion combines information from multiple sources to create a coherent and unified KG. This involves resolving conflicts and redundancies among the sources, such as contradictory or duplicate facts, and aggregating or reconciling the information based on rules, probabilities, or semantic similarity.\\n\\nWe utilized a robust methodology for creating KG triplets from unstructured text data, specifically focusing on corporate documents such as earnings call transcripts, adapted from Ref. [18? ]. This process involves several interconnected stages, each designed to extract, refine, and structure information effectively.\\n\\nWe implement a two-tiered LLM chain for content refinement and information extraction. The first tier employs an LLM to generate an abstract representation of each document chunk. This refinement process is crucial as it distills the essential information while preserving the original meaning and key relationships between concepts that serves as a more focused input for subsequent processing, enhancing the overall efficiency and accuracy of our\\n\\n\\x0ccontextual data. This combined context is then utilized as input for a LLM to generate the final responses. Details on the implementation of the HybridRAG will be provided in Section 4.4.\\n\\n2.4 Evaluation Metrics To assess the efficacy of this integrated approach, we conducted a comparative analysis among the three approaches in a controlled experimental set up: VectorRAG, GraphRAG and HybridRAG. The responses generated using the combined VectorRAG and GraphRAG contexts were juxtaposed against those produced individually by VectorRAG and GraphRAG. This comparative evaluation aimed to discern potential improvements in response quality, accuracy, and comprehensiveness that might arise from the synergistic integration of these two RAG methodologies.\\n\\nTo objectively evaluate different RAG approaches (VectorRAG and GraphRAG in their case), Ref. [22] utilized metrics such as comprehensiveness (i.e., the amount of details the answer provides to cover all aspects and details of the question?); diversity (i.e., the richness of the answer in providing different perspectives and insights on the question); empowerment (i.e., the helpfulness of the answer to the reader understand and make informed judgements about the topic); and, directness (i.e., clearness of the answer in addressing the question). here, the LLM was provided tuples of question, target metric, and a pair of answers, and was asked to assess which answer was better according to the metric and why. These metrics though compare the final generated answers, do not necessarily directly evaluate the retrieval and generation parts separately. Instead, here we implement a comprehensive set of evaluation metrics which are designed to capture different aspects of a given RAG system‚Äôs output quality, focusing on faithfulness, answer relevance, and context relevance [30]. Each metric provides unique insights into the system‚Äôs capabilities and limitations.\\n\\nFaithfulness. Faithfulness is a crucial metric that measures 2.4.1 the extent to which the generated answer can be inferred from the provided context. Our implementation of the faithfulness metric involves a two-step process: Statement Extraction:- We use an LLM to decompose the generated answer into a set of concise statements. This step is crucial for breaking down complex answers into more manageable and verifiable units. The prompt used for this step is:\\n\\n\"Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer]\". Statement Verification:- For each extracted statement, we employ the LLM to determine if it can be inferred from the given context. This verification process uses the following prompt:\\n\\n\"Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. statement: [statement 1] ... statement: [statement n]\".\\n\\nThe faithfulness score (ùêπ ) is ùêπ = |ùëâ |/|ùëÜ |, where |ùëâ | is the number of supported statements and |ùëÜ | is the total number of statements.\\n\\nSarmah et al.\\n\\n2.4.2 Answer Relevance:-. The answer relevance metric assesses how well the generated answer addresses the original question, irrespective of factual accuracy. This metric helps identify cases of incomplete answers or responses containing irrelevant information. Our implementation involves the following steps:\\n\\nQuestion Generation: We prompt the LLM to generate n potential\\n\\nquestions based on the given answer:\\n\\n\"Generate a question for the given answer. answer: [answer]\". Then, we obtain embeddings for all generated questions and the original question using OpenAI‚Äôs text-embedding-ada-002 model2. We then calculate the cosine similarity between each generated question‚Äôs embedding and the original question‚Äôs embedding.\\n\\nFinally, the answer relevance score (AR) is computed as the aver(cid:205)(ùë†ùëñùëö(ùëû, ùëûùëñ )), age similarity across all generated questions: ùê¥ùëÖ = 1 ùëõ where ùë†ùëñùëö(ùëû, ùëûùëñ ) is the cosine similarity between the embedding of the original question ùëû and the embeddings of each of the ùëõ generated questions ùëûùëñ .\\n\\n2.4.3 Context Precision. It is a metric used to evaluate the relevance of retrieved context chunks in relation to a specified ground truth for a given question3. It calculates the proportion of relevant items that appear in the top ranks of the context. The formula for context precision at K is the sum of the products of precision at each rank k and a binary relevance indicator v_k, divided by the total number of relevant items in the top K results. Precision at each rank k is determined by the ratio of true positives at k to the sum of true positives and false positives at k. This metric helps in assessing how well the context supports the ground truth, aiming for higher scores which indicate better precision.\\n\\n2.4.4 Context Recall. It is a metric used to evaluate how well the retrieved context aligns with the ground truth answer, which is considered the definitive correct response4. It is quantified by comparing each sentence in the ground truth answer to see if it can be traced back to the retrieved context. The formula for context recall is the ratio of the number of ground truth sentences that can be attributed to the context to the total number of sentences in the ground truth. Higher values, ranging from 0 to 1, indicate better alignment and thus better context recall. This metric is crucial for assessing the effectiveness of information retrieval systems in providing relevant context.\\n\\n3 DATA DESCRIPTION Although there do exist some public financial datasets, none of them were suitable for the present experiments: e.g., FinQA [31], TATQA [32], FIQA [33], FinanceBench [34], etc. datasets are limited to specific usecases such as benchmarking LLMs‚Äô abilities to perform complex numerical reasoning or sentiment analysis. On the other hand, FinTextQA [35] dataset was not publicly available at the time of writing the present work. In addition, in most of these datasets, access to the actual documents from which the groundtruth Q&As were created is not available, making it impossible to use them for our RAG techniques evaluation purposes. Hence, we resorted to a dataset of our own though through publicly available\\n\\n2https://platform.openai.com/docs/guides/embeddings/embedding-models 3https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html 4https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html\\n\\n\\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\n\\ndocuments but such that we finally have access to both the actual financial documents and the ground-truth Q&As. Datasets like FinanceBench5 provides question-context-answer triplets but they are not useful as here we are comparing VectorRAG, GraphRAG and HybridRAG and they do not provide the context generated from a KG. A recent paper [22] has not made the KG and triplets constructed by their algorithm public to the best of our knowledge either.\\n\\nIn short, there is no publicly available benchmark dataset to compare VectorRAG and GraphRAG either for financial or general domains to the best of our knowledge. Hence, we had to rely on our own dataset constructed as explained below.\\n\\nWe used transcripts from earnings calls of Nifty 50 constituents for our analysis. The NIFTY 50 is popular index in the Indian stock market that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange (NSE). The dataset of the earning call documents of Nifty 50 companies is widely recognized in the investment realm and is esteemed as an authoritative and extensive collection of earnings call transcripts. In our investigation, we focus on data spanning the quarter ending in June, 2023 i.e. the earnings reports for Q1 of the financial year 2024 (A financial year in India starts on the 1st April and ends in 31st March, so the quarter from 1st April to 30th June is the first quarter of 2024 for the Indian market).\\n\\nOur dataset encompasses 50 transcripts for this quarter, spanning over 50 companies within Nifty 50 universe from diverse range of sectors including Infrastructure, Healthcare, Consumer Durables, Banking, Automobile, Financial Services, Energy - Oil & Gas, Telecommunication, Consumer Goods, Pharmaceuticals, Energy - Coal, Materials, Information Technology, Construction, Diversified, Metals, Energy - Power and Chemicals providing a substantial and diverse foundation for our study.\\n\\nWe start the data collection process focused on acquiring earnings reports from company websites within the Nifty 50 universe by developing and deploying a custom web scraping tool to navigate through the websites of each company within the Nifty 50 index, systematically retrieving the pertinent earnings reports for Q1 of the financial year 2024. By utilizing this web scraping approach, we aimed to compile a comprehensive dataset encompassing the earnings reports of the constituent companies.\\n\\nTable 1 summarizes basic statistics of the documents we will be\\n\\nexperimenting with in the remainder of this work.\\n\\nNumber of companies/documents Average number of pages Average number of questions Average number of tokens\\n\\n50 27 16 60,000\\n\\nTable 1: Summary Statistics for the call transcript documents used in the present work.\\n\\nThese call transcripts documents consist of questions and answers between financial analysts and the company representatives for the respective companies, hence, there already exist certain Q&A pairs within these documents along with additional text. We examined the earnings reports within the Nifty50 universe, systematically curated a comprehensive array of randomly selected 400\\n\\n5https://huggingface.co/datasets/PatronusAI/financebench-test\\n\\nquestions posed during the earnings calls from all the documents, and gathered the exact responses corresponding to these questions. These questions constitute the specific queries articulated by financial analysts to the management during these calls.\\n\\n4 IMPLEMENTATION DETAILS In this Section, we provide details of implementation of the proposed methodology.\\n\\n4.1 Knowledge Graph Construction The initial phase of our approach centers on document preprocessing. We utilize the PyPDFLoader6 to import PDF documents, which are subsequently segmented into manageable chunks using the RecursiveCharacterTextSplitter. This chunking strategy employs a size of 2024 characters with an overlap of 204 characters, ensuring comprehensive coverage while maintaining context across segment boundaries.\\n\\nFollowing the preprocessing stage, we implement the two-tiered language model chain for content refinement and information extraction. It is not possible to include the exact prompt here due to the limited space, but a baseline prompt can be found in Ref. [18].\\n\\nEntity Type Companies and Corporations Financial Metrics and Indicators Corporate Executives and Key Personnel Products and Services\\n\\nExamples Official names, abbreviations, informal references Revenue, profit margins, EBITDA\\n\\nCEOs, CFOs, board members\\n\\nTangible products and intangible services\\n\\nGeographical Locations Headquarters, operational regions,\\n\\nCorporate Events\\n\\nmarkets Mergers, launches, earnings calls Legal cases, regulatory compliance\\n\\nacquisitions,\\n\\nproduct\\n\\nLegal and Regulatory Information Table 2: Entities extracted from earnings call transcripts\\n\\nTable 2 summarizes details on entities extracted from the earning calls transcripts using our prompt based method. Concurrently, LLM identifies relationships between these entities using a curated set of verbs, capturing the nuanced interactions within the corporate narrative. A key improvement in our methodology is the enhanced prompt engineering to generate the structured output format for knowledge triplets. Each triplet is represented as a nested list [‚Äôh‚Äô, ‚Äôtype‚Äô, ‚Äôr‚Äô, ‚Äôo‚Äô, ‚Äôtype‚Äô, ‚Äômetadata‚Äô], where ‚Äôh‚Äô and ‚Äôo‚Äô denote the head and object entities respectively, ‚Äôtype‚Äô specifies the entity category, ‚Äôr‚Äô represents the relationship, and ‚Äômetadata‚Äô encapsulates additional contextual information. This format allows for a rich, multidimensional representation of information, facilitating more nuanced downstream analysis.\\n\\n6https://python.langchain.com/v0.1/docs/modules/data_connection/document_ loaders/pdf/\\n\\n\\x0cOur process incorporates several advanced features to enhance the quality and utility of the extracted triplets. Entity disambiguation techniques are employed to consolidate different references to the same entity, improving consistency across the KG. We also prioritize conciseness in entity representation, aiming for descriptions of less than four words where possible, which aids in maintaining a clean and navigable graph structure.\\n\\nThe extraction pipeline is applied iteratively to each document chunk, with results aggregated to form a comprehensive knowledge graph representation of the entire document, allowing for scalable processing of large documents while maintaining local context within each chunk. We have added explicit instruction on obtaining metadata following Ref. [8] for both VectorRAG and GraphRAG.\\n\\nFinally, we implement a data persistence strategy, converting the extracted triplets from their initial string format to Python data structures and storing them in a pickle file. This facilitates easy retrieval and further manipulation of the knowledge graph data in subsequent analysis stages.\\n\\nOur methodology represents a significant advancement in automated knowledge extraction from corporate documents. By combining advanced NLP techniques with a structured approach to information representation, we create a rich, queryable knowledge base that captures the complex relationships and key information present in corporate narratives. This approach opens up new possibilities for financial analysis and automated reasoning in the business domain that will be explored further in the future.\\n\\n4.2 VectorRAG Our methodology builds upon the concept of RAG [9] which allows for the creation of a system that can provide context-aware, accurate responses to queries about company financial information, leveraging both the power of large language models and the efficiency of semantic search.\\n\\nAt the core of our system is a Pinecone vector database7, which serves as the foundation for our information retrieval process. We employ OpenAI‚Äôs text-embedding-ada-002 model to transform textual data from earnings call transcripts into high-dimensional vector representations. This vectorization process enables semantic similarity searches, significantly enhancing the relevance and accuracy of retrieved information. Table 3 provides summary of the configuration of the set up in use for our experiments.\\n\\nLLM LLM Temperature Embedding Model Framework Vector Database Chunk Size Chunk Overlap Maximum Output Tokens Chunks for Similarity Algorithm 20 Number of Context Retrieved 4\\n\\nGPT-3.5-Turbo 0 text-embedding-ada-002 LangChain Pinecone 1024 0 1024\\n\\nTable 3: VectorRAG Configuration\\n\\nSarmah et al.\\n\\nThe Q&A pipeline is constructed using the LangChain framework8. The begins with a context retrieval step, where we query the Pinecone vector store to obtain the most relevant document chunks for a given question. This retrieval process is fine-tuned with specific filters for quarters, years, and company names, ensuring that the retrieved information is both relevant and current.\\n\\nFollowing retrieval, we implement a context formatting step that consolidates the retrieved document chunks into a coherent context string. This formatted context serves as the informational basis for the language model‚Äôs response generation. We have developed a sophisticated prompt template, that instructs the language model to function as an expert Q&A system, emphasizing the importance of utilizing only the provided context information and avoiding direct references to the context in the generated responses.\\n\\nFor the core language processing task, we integrate OpenAI‚Äôs GPT-3.5-turbo model which processes the formatted context and query to generate natural language responses that are informative, coherent, and contextually appropriate.\\n\\nTo evaluate the performance of our system, we developed a comprehensive framework that includes the preparation of a custom dataset of question-answer pairs specific to each company‚Äôs earnings call. Our system processes each question in this dataset, generating answers based on the retrieved context. The evaluation results, including the original question, generated answer, retrieved contexts, and ground truth, are compiled into structured formats (CSV and JSON) to facilitate further analysis. The outputs generated by our system are stored in both CSV and JSON formats, enabling easy integration with various analysis tools and dashboards. This approach facilitates both quantitative performance metrics and qualitative assessment of the system‚Äôs responses, providing a comprehensive view of its effectiveness.\\n\\nBy parameterizing company names, quarters, and years, we can easily adapt the system to different datasets and time periods. This design choice allows for seamless integration of new data and expansion to cover multiple companies and earnings calls.\\n\\n4.3 GraphRAG For GraphRAG, we developed an Q&A system specifically designed for corporate earnings call transcripts. Our implementation of GraphRAG leverages several key components and techniques:\\n\\nLLM LLM Temperature Framework KG Manipulation Chunk Size Chunk Overlap Number of Triplets Number of nodes Number of edges DFS Depth Table 4: GraphRAG Configuration\\n\\nGPT-3.5-Turbo 0 LangChain Networkx 1024 0 13950 11405 13883 1\\n\\n7https://www.pinecone.io/\\n\\n8https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain\\n\\n\\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\n\\nKnowledge Graph Construction:- We begin by constructing a KG from a set of knowledge triplets extracted from corporate documents using the prompt engineering based methodology as described in Section 4.1. These triplets, stored in a pickle file, represent structured information in the form of subject-predicate-object relationships. We use the NetworkxEntityGraph class from the LangChain library to create and manage this graph structure. Each triple is added to the graph, which encapsulates the head entity, relation, tail entity, and additional metadata.\\n\\nWe implement the Q&A functionality using the GraphQAChain class from LangChain. This chain combines the KG with an LLM (in our case, OpenAI‚Äôs GPT-3.5-turbo) to generate responses. The GraphQAChain traverses the KG to find relevant information and uses the language model to formulate coherent answers based on the retrieved context. A summary of configuration of our LLM models and other libraries used for GraphRAG is shown in Table 4. In KG as the information is stored in the form of entities and relationships and there can be multiple relations emanating from one single entity, in this experiment, to extract relevant information from the KG, we employ a depth-first search strategy constrained by a depth of one from the specified entity.\\n\\nTo prepare for assessing the performance of our GraphRAG system, we follow the below steps: Dataset Preparation:- We use a pre-generated CSV file containing question-answer pairs specific to the earnings call transcript. Iterative Processing:- For each question in the dataset, we run the GraphQAChain to generate an answer. Result Compilation:- We compile the results, including the original questions, generated answers, retrieved contexts, and ground truth answers, into a structured format.\\n\\nFinally, the evaluation results are saved in both CSV and JSON formats for further analysis and comparison. We then fed these outputs into our RAG evaluation pipeline. For each Q&A pair in our dataset, we compute all three metrics: faithfulness, answer relevance, context precision and context recall.\\n\\n4.4 HybridRAG For the proposed HybridRAG technique, upon obtaining all the contextual information from VectorRAG and GraphRAG, we concatenate these contexts to form a unified context utilizing both techniques. This combined context is then fed into the answer generator model to produce a response. The context used for response generation is relatively larger, which affects the precision of the generated response. The context from VectorRAG is appended first, followed by the context from GraphRAG. Consequently, the precision of the generated answer depends on the source context. If the answer is generated from the GraphRAG context, it will have lower precision, as the GraphRAG context is added last in the sequence of contexts provided to the answer generator model, and vice versa.\\n\\n5 RESULTS We evaluate both the retrieval and generation parts of RAG for the three different RAG pipelines. Evaluating the RAG outputs is also an active area of research there is no standard tool which is universally accepted as of yet, though we use a currently popular framework RAGAS[30] to evaluate the three RAG pipelines in the\\n\\npresent work where we have modified them a bit to make more precise comparisons.\\n\\nThe results of our comparative analysis reveal notable differences in performance among VectorRAG, GraphRAG, and HybridRAG approaches as summarized in Table 5. In terms of Faithfulness, GraphRAG and HybridRAG demonstrated superior performance, both achieving a score of 0.96, while VectorRAG trailed slightly with a score of 0.94. Answer relevancy scores varied across the methods, with HybridRAG outperforming the others at 0.96, followed by VectorRAG at 0.91, and GraphRAG at 0.89. Context precision was highest for GraphRAG at 0.96, significantly surpassing VectorRAG (0.84) and HybridRAG (0.79). However, in context recall, both VectorRAG and HybridRAG achieved perfect scores of 1, while GraphRAG lagged behind at 0.85.\\n\\nOverall, these results suggest that GraphRAG offers improvements over VectorRAG, particularly in faithfulness and context precision. Furthermore, HybridRAG emerges as the most balanced and effective approach, outperforming both VectorRAG and GraphRAG in key metrics such as faithfulness and answer relevancy, while maintaining high context recall.\\n\\nThe relatively lower context precision observed for HybridRAG (0.79) can be attributed to its unique approach of combining contexts from both VectorRAG and GraphRAG methods. While this integration allows for more comprehensive information retrieval, it also introduces additional content that may not align precisely with the ground truth, thus affecting the context precision metric. Despite this trade-off, HybridRAG‚Äôs superior performance in faithfulness, answer relevancy, and context recall underscores its effectiveness. When considering the overall evaluation metrics, HybridRAG emerges as the most promising approach, balancing high-quality answers with comprehensive context retrieval.\\n\\nOverall GraphRAG performs better in extractive questions compared to VectorRAG. And VectorRAG does better in abstractive questions where information is not explicitly mentioned in the raw data. And also GraphRAG sometimes fails to answer questions correctly whenever there is no entity explicitly mentioned in the question. So HybridRAG does a good job overall, as whenever VectorRAG fails to fetch correct context in extractive questions it falls back to GraphRAG to generate the answer. And whenever GraphRAG fails to fetch correct context in abstractive questions it falls back to VectorRAG to generate the answer.\\n\\nVectorRAG GraphRAG HybridRAG 0.96 0.89 0.96 0.85\\n\\n0.96 0.96 0.79 1\\n\\n0.94 0.91 0.84 1\\n\\nF AR CP CR\\n\\nTable 5: Performance Metrics for Different RAG Pipelines. Here, F, AR, CP and CR refer to Faithfulness, Answer Relevence, Context Precision and Context Recall.\\n\\n6 CONCLUSION AND FUTURE DIRECTIONS Among the current approaches to mitigate issues regarding information extraction from external documents that were not part of\\n\\n\\x0ctraining data for the LLM, Retrieval Augmented Generation (RAG) techniques have emerged as the most popular ones that aim to improve the performance of LLMs by incorporating relevant retrieval mechanisms. RAG methods enhance the LLMs‚Äô capabilities by retrieving pertinent documents or text to provide additional context during the generation process. However, these approaches encounter significant limitations when applied to the specialized and intricate domain of financial documents. Furthermore, the quality of the retrieved context from a vast and heterogeneous corpus can be inconsistent, leading to inaccuracies and incomplete analyses. These challenges highlight the need for more sophisticated methods that can effectively integrate and process the detailed and domainspecific information found in financial documents, ensuring more reliable and accurate outputs for informed decision-making.\\n\\nIn the present work, we have introduced a novel approach that significantly advances the field of information extraction from financial documents through the development of a hybrid RAG system. This system, called HybridRAG, which integrates the strengths of both Knowledge Graphs (KGs) and advanced language models, represents a leap forward in our ability to extract and interpret complex information from unstructured financial texts. The hybrid RAG system, by combining traditional vector-based RAG and KGbased RAG, has shown superior performance in terms of retrieval accuracy and answer generation, marking a pivotal step towards more effective financial analysis tools.\\n\\nThrough a comparative analysis using objecive evaluation metrics, we have highlighted the distinct performance advantages of the HybridRAG approach over its vector-based and KG-based counterparts. The HybridRAG system excels in faithfulness, answer relevancy, and context recall, demonstrating the benefits of integrating contexts from both VectorRAG and GraphRAG methods, despite potential trade-offs in context precision.\\n\\nThe implications of this research extend beyond the immediate realm of financial analysis. By developing a system capable of understanding and responding to nuanced queries about complex financial documents, we pave the way for more sophisticated AI-assisted financial decision-making tools that could potentially democratize access to financial insights, allowing a broader range of stakeholders to engage with and understand financial information. Future directions for this research include expanding the system to handle multi-modal inputs, incorporating numerical data analysis capabilities, and developing more sophisticated evaluation metrics that capture the nuances of financial language and the accuracy of numerical information in the responses. Additionally, exploring the integration of this system with real-time financial data streams could further enhance its utility in dynamic financial environments.\\n\\n7 ACKNOWLEDGEMENT The views expressed here are those of the authors alone and not of BlackRock, Inc or NVIDIA. We are grateful to Emma Lind for her invaluable support for this collaboration.\\n\\nREFERENCES [1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation\\n\\nSarmah et al.\\n\\n[3] Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: A pretrained language model for financial communications. arXiv preprint arXiv:2006.08097, 2020.\\n\\n[4] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.\\n\\n[5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qingsong Wen, and Stefan Zohren. A survey of large language models for financial applications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903, 2024.\\n\\n[6] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, and Tianming Liu. Revolutionizing finance with llms: An overview of applications and insights, 2024.\\n\\n[7] Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv preprint arXiv:2305.18703, 2023.\\n\\n[8] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. Towards reducing hallucination in extracting information from financial reports using large language models. In Proceedings of the Third International Conference on AI-ML Systems, pages 1‚Äì5, 2023.\\n\\n[9] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459‚Äì9474, 2020. [10] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2021.\\n\\n[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. arXiv preprint\\n\\nRealm: Retrieval-augmented language model pre-training. arXiv:2002.08909, 2020.\\n\\n[12] Antonio Jose Jimeno Yepes et al. Financial report chunking for effective retrieval\\n\\naugmented generation. arXiv preprint arXiv:2402.05131, 2024.\\n\\n[13] SuperAcc. Retrieval-augmented generation on financial statements. SuperAcc\\n\\nInsights, 2024.\\n\\n[14] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494‚Äì514, 2021. [15] Heiko Paulheim. Knowledge graphs: State of the art and future directions.\\n\\nSemantic Web Journal, 10(4):1‚Äì20, 2017.\\n\\n[16] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D‚ÄôAmato, Gerard de Melo, Claudio Guti√©rrez, and Maria ... Maleshkova. Knowledge graphs. arXiv preprint arXiv:2003.02320, 2021.\\n\\n[17] Lisa Ehrlinger and Wolfram W√∂√ü. Towards a definition of knowledge graphs. In\\n\\nSEMANTiCS (Posters, Demos, SuCCESS), pages 1‚Äì4, 2016.\\n\\n[18] Xiaohui Victor Li and Francesco Sanna Passino. Findkg: Dynamic knowledge graphs with large language models for detecting global trends in financial markets. arXiv preprint arXiv:2407.10909, 2024.\\n\\n[19] Shourya De, Anima Aggarwal, and Anna Cinzia Squicciarini. Financial knowledge graphs: A novel approach to empower data-driven financial applications. In IEEE International Conference on Big Data (Big Data), pages 1311‚Äì1320, 2018. [20] Marina Petrova and Birgit Reinwald. Knowledge graphs in finance: Applications\\n\\nand opportunities. Journal of Financial Data Science, 2(2):10‚Äì19, 2020.\\n\\n[21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data\\n\\nintegration and analysis. Data Science Journal, 18(1):1‚Äì15, 2019.\\n\\n[22] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\\n\\n[23] Xuchen Yao, Yanan Sun, Zhen Huang, and Dong Li. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2101.07554, 2021. [24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrievalaugmented generation for open-domain question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3098‚Äì3106, 2022.\\n\\n[25] Bill Yuchen Lin, Yuanhe Liu, Ming Shen, and Xiang Ren. Kgpt: Knowledgegrounded pre-training for data-to-text generation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 710‚Äì724, 2020.\\n\\n[26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.\\n\\n[27] Tyler Procko. Graph retrieval-augmented generation for large language models:\\n\\nof word representations in vector space. 2013.\\n\\nA survey. Available at SSRN, 2024.\\n\\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\n[28] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):1‚Äì62, 2023.\\n\\n\\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction\\n\\n[29]\\n\\nIshani Mondal, Yufang Hou, and Charles Jochim. End-to-end nlp knowledge graph construction. arXiv preprint arXiv:2106.01167, 2021.\\n\\n[30] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023.\\n\\n[31] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122, 2021.\\n\\n[32] Macedo Maia, Siegfried Handschuh, Andr√© Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www‚Äô18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the\\n\\nweb conference 2018, pages 1941‚Äì1942, 2018.\\n\\n[33] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624, 2021.\\n\\n[34] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang. Fintextqa: A dataset for long-form financial question answering. arXiv preprint arXiv:2405.09980, 2024.\\n\\n[35]\\n\\n\\x0c'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\\n\", \"\\n\\n\",\".\",\"\\n\"],  # Prioritize splitting by paragraphs, then sentences, then spaces\n",
    "    chunk_size = 500,  # Maximum size of each chunk\n",
    "    chunk_overlap = 100  # Overlap to maintain context between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chunk(chunk):\n",
    "    cleaned_chunk = chunk.strip() # remove leading/trailing whitespaces\n",
    "    cleaned_chunk = re.sub(r'^\\.+', '', cleaned_chunk) # remove leading fullstops\n",
    "    cleaned_chunk = re.sub(r'\\n+', ' ', cleaned_chunk) # remove \\n\\n\n",
    "    cleaned_chunk = cleaned_chunk.strip()\n",
    "    return cleaned_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_chunk = [clean_chunk(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4 2 0 2 g u A 9 ] L C . s c [ 1 v 8 4 9 4 0 . 8 0 4 2 : v i X r a HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction Bhaskarjit Sarmah bhaskarjit.sarmah@blackrock.com BlackRock, Inc. Gurugram, India Benika Hall bhall@nvidia.com NVIDIA Santa Clara, CA, USA Rohan Rao rohrao@nvidia.com NVIDIA Santa Clara, CA, USA Sunil Patel supatel@nvidia.com NVIDIA Santa Clara, CA, USA',\n",
       " 'Sunil Patel supatel@nvidia.com NVIDIA Santa Clara, CA, USA Stefano Pasquali stefano.pasquali@blackrock.com BlackRock, Inc. New York, NY, USA Dhagash Mehta dhagash.mehta@blackrock.com BlackRock, Inc. New York, NY, USA',\n",
       " 'ABSTRACT Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents',\n",
       " 'We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers',\n",
       " 'Using experiments on a set of financial earning call transcripts documents which come in the form of Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation',\n",
       " 'The proposed technique has applications beyond the financial domain.',\n",
       " '1 INTRODUCTION For the financial analyst, it is crucial to extract and analyze information from unstructured data sources like news articles, earnings reports, and other financial documents to have at least some chance to be on the better side of potential information asymmetry. These sources hold valuable insights that can impact investment decisions, market predictions, and overall sentiment',\n",
       " 'However, traditional data analysis methods struggle to effectively extract and utilize this information due to its unstructured nature. Large language models (LLMs) [1‚Äì4] have emerged as powerful tools for financial services and investment management. Their ability to process and understand vast amounts of textual data makes them invaluable for tasks such as sentiment analysis, market trend predictions, and automated report generation',\n",
       " 'Specifically, extracting information from annual reports and other financial documents can greatly enhance the efficiency and accuracy of financial analysts [5]. A robust information extraction system can help analysts quickly gather relevant data, identify market trends, and make informed decisions, leading to better investment strategies and risk management [6].',\n",
       " 'Although LLMs have substantial potential in financial applications, there are notable challenges in using pre-trained models to extract information from financial documents outside their training data while also reducing hallucination [7, 8]. Financial documents typically contain domain-specific language, multiple data formats, and unique contextual relationships that general purpose-trained LLMs do not handle well',\n",
       " 'In addition, extracting consistent and coherent information from multiple financial documents can be challenging due to variations in terminology, format, and context across different textual sources. The specialized terminology and complex data formats in financial documents make it difficult for models to extract meaningful insights, in turn, causing inaccurate predictions, overlooked insights, and unreliable analysis, which ultimately hinder the ability to make well-informed decisions.',\n",
       " 'Current approaches to mitigate these issues include various Retrieval-Augmented Generation (RAG) techniques [9], which aim to improve the performance of LLMs by incorporating relevant retrieval techniques. VectorRAG (the traditional RAG techniques that are based on vector databases) focuses on improving Natural Language Processing (NLP) tasks by retrieving relevant textual information to support the generation tasks',\n",
       " 'VectorRAG excels in situations where context from related textual documents is crucial for generating meaningful and coherent responses [9‚Äì11]. RAGbased methods ensure the LLMs generate relevant and coherent responses that are aligned with the original input query. However, for financial documents, these approaches have significant challenges as a standalone solution',\n",
       " 'For instance, traditional RAG systems often use paragraph-level chunking techniques, which assume the text in those documents are uniform in length. This approach neglects the hierarchical nature of financial statements and can result in the loss of critical contextual information for an accurate analysis[12, 13]',\n",
       " 'Due to the complexities in analyzing financial documents, the quality of the LLM retrieved-context from a vast and heterogeneous corpus can be inconsistent, leading to inaccuracies and incomplete analyses. These challenges demonstrate the need for more sophisticated methods that can effectively integrate and process the detailed and domain-specific information found in financial documents, ensuring more reliable and accurate results for informed decision-making.',\n",
       " 'Knowledge graphs (KGs) [14] may provide a different point of view to looking at the financial documents where the documents',\n",
       " 'are viewed as a collection of triplets of entities and their relationships as depicted in the text of the documents. KGs have become a pivotal technology in data management and analysis, providing a structured way to represent knowledge through entities and relationships and have been widely adopted in various domains, including search engines, recommendation systems, and biomedical research [15‚Äì17]',\n",
       " 'The primary advantage of KGs lies in their ability to offer a structured representation, which facilitates efficient querying and reasoning. However, building and maintaining KGs and integrating data from different sources, such as documents, news articles, and other external sources, into a coherent knowledge graph poses significant challenges.',\n",
       " 'The financial services industry has recognized the potential of KGs in enhancing data integration of heterogeneous data sources, risk management, and predictive analytics [18? ‚Äì21]. Financial KGs integrate various financial data sources such as market data, financial reports, and news articles, creating a comprehensive view of financial entities and their relationships',\n",
       " 'This unified view improves the accuracy and comprehensiveness of financial analysis, facilitates risk management by identifying hidden relationships, and supports advanced predictive analytics for more accurate market predictions and investment decisions. However, handling large volumes of financial data and continuously updating the knowledge graph to reflect the dynamic nature of financial markets can be challenging and resource-intensive.',\n",
       " 'GraphRAG (Graph-based Retrieval-Augmented Generation) [22‚Äì 27] is a novel approach that leverages knowledge graphs (KGs) to enhance the performance of NLP tasks such as Q&A systems. By integrating KGs with RAG techniques, GraphRAG enables more accurate and context-aware generation of responses based on the structured information extracted from financial documents. But GraphRAG generally underperforms in abstractive Q&A tasks or when there is not explicit entity mentioned in the question.',\n",
       " 'In the present work, we propose a combination of VectorRAG and GraphRAG, called HybridRAG, to retrieve the relevant information from external documents for a given query to the LLM that brings advantages of both the RAGs together to provide demonstrably more accurate answers to the queries.',\n",
       " '1.1 Prior Work and Our Contribution VectorRAG has been extensively investigated in the recent years and focuses on enhancing NLP tasks by retrieving relevant textual information to support generation processes [9‚Äì11, 26]. However, the effectiveness of the retrieval mechanism across multiple documents and longer contexts poses a significant challenge in extracting relevant responses. GraphRAG combines the capabilities of KGs with RAG to improve traditional NLP tasks [23‚Äì25]',\n",
       " 'GraphRAG combines the capabilities of KGs with RAG to improve traditional NLP tasks [23‚Äì25]. Within our implementations of both VectorRAG GraphRAG techniques, we explicitly add information on the metadata of the documents that is also shown to improve the performance of VectorRAG [8].',\n",
       " 'To the best of our knowledge the present work is the first work that proposes a RAG approach that is hybrid of both VectorRAG and GraphRAG and demonstrates its potential of more effective analysis and utilization of financial documents by leveraging the combined strengths of VectorRAG and GraphRAG. We also utilize a novel ground-truth Q&A dataset extracted from publicly available Sarmah et al.',\n",
       " 'Sarmah et al. financial call transcripts of the companies included in the Nifty-50 index which is an Indian stock market index that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange1.',\n",
       " '2 METHODOLOGY In this Section, we provide details of the proposed methodology by first discussing details of VectorRAG, then methodologies of constructing KGs from given documents and our proposed methodology of GraphRAG and finally the HybridGraph technique.',\n",
       " '2.1 VectorRAG The traditional RAG [9] process begins with a query that is related to the information possessed within external document(s) that are not a part of the training dataset for the LLM. This query is used to search an external repository, such as a vector database or indexed corpus, to fetch relevant documents or passages containing useful information. These retrieved documents are then fed back into the LLM as additional context',\n",
       " 'These retrieved documents are then fed back into the LLM as additional context. Hence, in turn, for the given query, the language model generates a response based not only on its internal training data but also by incorporating the retrieved external information. This integration ensures that the generated content is grounded in more recent and verifiable data, improving the accuracy and contextual relevance of the responses',\n",
       " 'By combining the retrieval of external information with the generative capabilities of large language models, RAG enhances the overall quality and reliability of the generated text.',\n",
       " 'In traditional VectorRAG, the given external documents are divided into multiple chunks because of the limitation of context size of the language model. Those chunks are converted into embeddings using an embeddings model and then stored into a vector database. After that, the retrieval component performs a similarity search within the vector database to identify and rank the chunks most relevant to the query',\n",
       " 'The top-ranked chunks are retrieved and aggregated to provide context for the generative model.',\n",
       " 'Then, the generative model takes this retrieved context along with the original query and synthesizes a response. Thus, it merges the real-time information from the retrieved chunks with its preexisting knowledge, ensuring that the response is both contextually relevant and detailed.',\n",
       " 'The schematic diagram in Figure 1 provides details on the part of RAG that generates vector database from given external documents in the traditional VectorRAG methodology where we also include explicit reference of metadata [8]. Section 4.2 will provide implementation details for our experiments.',\n",
       " '2.2 Knowledge Graph Construction A KG is a structured representation of real-world entities, their attributes, and their relations, usually stored in a graph database or a triplet store, i.e., a KG consists of nodes that represent entities and edges that represent relations, as well as labels and attributes for both. A graph triplet is a basic unit of information in a KG, consisting of a subject, a predicate, and an object.',\n",
       " 'In most methodologies to build a KG from given documents, three main steps are involved: knowledge extraction, knowledge 1https://www.nseindia.com/products/content/equities/indices/nifty_50.htm \\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction triplet extraction pipeline. The second tier of our LLM chain is dedicated to entity extraction and relationship identification.',\n",
       " 'Both the steps are executed using carefully performed prompt engineering on a pre-trained LLM. A detailed discussion on implementation of the methodology will be provided in Section 4.1',\n",
       " '2.3 GraphRAG KG based RAG [22], or GraphRAG, also begins with a query based on the user‚Äôs input same as VectorRAG. The main difference between VectorRAG and GraphRAG lies in the retrieval part. The query here is now used to search the KG to retrieve relevant nodes (entities) and edges (relationships) related to the query. A subgraph, which consists of these relevant nodes and edges, is extracted from the full KG to provide context',\n",
       " 'This subgraph is then integrated with the language model‚Äôs internal knowledge, by encoding the graph structure into embeddings that the model can interpret. The language model uses this combined context to generate responses that are informed by both the structured information from the KG and its pre-trained knowledge',\n",
       " 'Crucially, when responding to user queries about a particular company, we leveraged the metadata information to selectively filter and retrieve only those document segments pertinent to the queried company [8]. This integration helps ensure that the generated outputs are accurate, contextually relevant, and grounded in verifiable information.',\n",
       " 'A schematic diagram of the retrieval methodology of GraphRAG is given in Figure 2. Here we first write a prompt to clean the data and then write another prompt in the second stage to create knowledge triplets along with metadata. It will be covered in more detail in section 4.1 Figure 2: A schematic diagram describing knowledge graph creation process of GraphRAG.',\n",
       " 'Figure 2: A schematic diagram describing knowledge graph creation process of GraphRAG. 2.3.1 HybridRAG. For the HybridRAG technique, we propose to integrate the aforementioned two distinct RAG techniques: VectorRAG and GraphRAG. This integration involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system, the latter of which was constructed specifically for this study.',\n",
       " 'The amalgamation of these two contexts allows us to leverage the strengths of both approaches. The VectorRAG component provides a broad, similarity-based retrieval of relevant information, while the GraphRAG element contributes structured, relationship-rich Figure 1: A schematic diagram describing the vector database creation of a RAG application.',\n",
       " 'improvement, and knowledge adaptation [28]. Here, we do not use knowledge adaptation and treat the KGs as static graphs. Knowledge Extraction:- This step aims to extract structured information from unstructured or semi-structured data, such as text, databases, and existing ontologies. The main tasks in this step are entity recognition, relationship extraction, and co-reference resolution',\n",
       " 'Entity recognition and relationship extraction techniques use typical NLP tasks to identify entities and their relationships from textual sources [29]. Coreference resolution identifies and connects different references of the same entity, keeping coherence within the knowledge graph. For example, if the text refers to a company as both \"the company\" and \"it\", coreference resolution can link these mentions to the same entity node in the graph',\n",
       " 'Knowledge Improvement:- This step aims to enhance the quality and completeness of the KG by removing redundancies and addressing gaps in the extracted information. The primary tasks in this step are KG completion and fusion. KG completion technique infers missing entities and relationships within the graph using methods such as link prediction and entity resolution',\n",
       " 'Link prediction predicts the existence and type of a relation between two entities based on the graph structure and features, while entity resolution matches and merges different representations of the same entity from different sources.',\n",
       " 'Knowledge fusion combines information from multiple sources to create a coherent and unified KG. This involves resolving conflicts and redundancies among the sources, such as contradictory or duplicate facts, and aggregating or reconciling the information based on rules, probabilities, or semantic similarity.',\n",
       " 'We utilized a robust methodology for creating KG triplets from unstructured text data, specifically focusing on corporate documents such as earnings call transcripts, adapted from Ref. [18? ]. This process involves several interconnected stages, each designed to extract, refine, and structure information effectively.',\n",
       " 'We implement a two-tiered LLM chain for content refinement and information extraction. The first tier employs an LLM to generate an abstract representation of each document chunk. This refinement process is crucial as it distills the essential information while preserving the original meaning and key relationships between concepts that serves as a more focused input for subsequent processing, enhancing the overall efficiency and accuracy of our',\n",
       " 'contextual data. This combined context is then utilized as input for a LLM to generate the final responses. Details on the implementation of the HybridRAG will be provided in Section 4.4.',\n",
       " '2.4 Evaluation Metrics To assess the efficacy of this integrated approach, we conducted a comparative analysis among the three approaches in a controlled experimental set up: VectorRAG, GraphRAG and HybridRAG. The responses generated using the combined VectorRAG and GraphRAG contexts were juxtaposed against those produced individually by VectorRAG and GraphRAG',\n",
       " 'This comparative evaluation aimed to discern potential improvements in response quality, accuracy, and comprehensiveness that might arise from the synergistic integration of these two RAG methodologies.',\n",
       " 'To objectively evaluate different RAG approaches (VectorRAG and GraphRAG in their case), Ref. [22] utilized metrics such as comprehensiveness (i.e., the amount of details the answer provides to cover all aspects and details of the question?); diversity (i.e., the richness of the answer in providing different perspectives and insights on the question); empowerment (i.e., the helpfulness of the answer to the reader understand and make informed judgements about the topic); and, directness (i.e',\n",
       " 'e., clearness of the answer in addressing the question). here, the LLM was provided tuples of question, target metric, and a pair of answers, and was asked to assess which answer was better according to the metric and why. These metrics though compare the final generated answers, do not necessarily directly evaluate the retrieval and generation parts separately',\n",
       " 'Instead, here we implement a comprehensive set of evaluation metrics which are designed to capture different aspects of a given RAG system‚Äôs output quality, focusing on faithfulness, answer relevance, and context relevance [30]. Each metric provides unique insights into the system‚Äôs capabilities and limitations.',\n",
       " 'Faithfulness. Faithfulness is a crucial metric that measures 2.4.1 the extent to which the generated answer can be inferred from the provided context. Our implementation of the faithfulness metric involves a two-step process: Statement Extraction:- We use an LLM to decompose the generated answer into a set of concise statements. This step is crucial for breaking down complex answers into more manageable and verifiable units. The prompt used for this step is:',\n",
       " '\"Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer]\". Statement Verification:- For each extracted statement, we employ the LLM to determine if it can be inferred from the given context. This verification process uses the following prompt:',\n",
       " '\"Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. statement: [statement 1] ... statement: [statement n]\".',\n",
       " 'The faithfulness score (ùêπ ) is ùêπ = |ùëâ |/|ùëÜ |, where |ùëâ | is the number of supported statements and |ùëÜ | is the total number of statements. Sarmah et al. 2.4.2 Answer Relevance:-. The answer relevance metric assesses how well the generated answer addresses the original question, irrespective of factual accuracy. This metric helps identify cases of incomplete answers or responses containing irrelevant information. Our implementation involves the following steps:',\n",
       " 'Question Generation: We prompt the LLM to generate n potential questions based on the given answer: \"Generate a question for the given answer. answer: [answer]\". Then, we obtain embeddings for all generated questions and the original question using OpenAI‚Äôs text-embedding-ada-002 model2. We then calculate the cosine similarity between each generated question‚Äôs embedding and the original question‚Äôs embedding.',\n",
       " 'Finally, the answer relevance score (AR) is computed as the aver(cid:205)(ùë†ùëñùëö(ùëû, ùëûùëñ )), age similarity across all generated questions: ùê¥ùëÖ = 1 ùëõ where ùë†ùëñùëö(ùëû, ùëûùëñ ) is the cosine similarity between the embedding of the original question ùëû and the embeddings of each of the ùëõ generated questions ùëûùëñ .',\n",
       " '2.4.3 Context Precision. It is a metric used to evaluate the relevance of retrieved context chunks in relation to a specified ground truth for a given question3. It calculates the proportion of relevant items that appear in the top ranks of the context. The formula for context precision at K is the sum of the products of precision at each rank k and a binary relevance indicator v_k, divided by the total number of relevant items in the top K results',\n",
       " 'Precision at each rank k is determined by the ratio of true positives at k to the sum of true positives and false positives at k. This metric helps in assessing how well the context supports the ground truth, aiming for higher scores which indicate better precision.',\n",
       " '2.4.4 Context Recall. It is a metric used to evaluate how well the retrieved context aligns with the ground truth answer, which is considered the definitive correct response4. It is quantified by comparing each sentence in the ground truth answer to see if it can be traced back to the retrieved context. The formula for context recall is the ratio of the number of ground truth sentences that can be attributed to the context to the total number of sentences in the ground truth',\n",
       " 'Higher values, ranging from 0 to 1, indicate better alignment and thus better context recall. This metric is crucial for assessing the effectiveness of information retrieval systems in providing relevant context.',\n",
       " '3 DATA DESCRIPTION Although there do exist some public financial datasets, none of them were suitable for the present experiments: e.g., FinQA [31], TATQA [32], FIQA [33], FinanceBench [34], etc. datasets are limited to specific usecases such as benchmarking LLMs‚Äô abilities to perform complex numerical reasoning or sentiment analysis. On the other hand, FinTextQA [35] dataset was not publicly available at the time of writing the present work',\n",
       " 'In addition, in most of these datasets, access to the actual documents from which the groundtruth Q&As were created is not available, making it impossible to use them for our RAG techniques evaluation purposes. Hence, we resorted to a dataset of our own though through publicly available',\n",
       " '2https://platform.openai.com/docs/guides/embeddings/embedding-models 3https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html 4https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html \\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction',\n",
       " 'documents but such that we finally have access to both the actual financial documents and the ground-truth Q&As. Datasets like FinanceBench5 provides question-context-answer triplets but they are not useful as here we are comparing VectorRAG, GraphRAG and HybridRAG and they do not provide the context generated from a KG. A recent paper [22] has not made the KG and triplets constructed by their algorithm public to the best of our knowledge either.',\n",
       " 'In short, there is no publicly available benchmark dataset to compare VectorRAG and GraphRAG either for financial or general domains to the best of our knowledge. Hence, we had to rely on our own dataset constructed as explained below.',\n",
       " 'We used transcripts from earnings calls of Nifty 50 constituents for our analysis. The NIFTY 50 is popular index in the Indian stock market that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange (NSE). The dataset of the earning call documents of Nifty 50 companies is widely recognized in the investment realm and is esteemed as an authoritative and extensive collection of earnings call transcripts',\n",
       " 'In our investigation, we focus on data spanning the quarter ending in June, 2023 i.e. the earnings reports for Q1 of the financial year 2024 (A financial year in India starts on the 1st April and ends in 31st March, so the quarter from 1st April to 30th June is the first quarter of 2024 for the Indian market).',\n",
       " 'Our dataset encompasses 50 transcripts for this quarter, spanning over 50 companies within Nifty 50 universe from diverse range of sectors including Infrastructure, Healthcare, Consumer Durables, Banking, Automobile, Financial Services, Energy - Oil & Gas, Telecommunication, Consumer Goods, Pharmaceuticals, Energy - Coal, Materials, Information Technology, Construction, Diversified, Metals, Energy - Power and Chemicals providing a substantial and diverse foundation for our study.',\n",
       " 'We start the data collection process focused on acquiring earnings reports from company websites within the Nifty 50 universe by developing and deploying a custom web scraping tool to navigate through the websites of each company within the Nifty 50 index, systematically retrieving the pertinent earnings reports for Q1 of the financial year 2024. By utilizing this web scraping approach, we aimed to compile a comprehensive dataset encompassing the earnings reports of the constituent companies.',\n",
       " 'Table 1 summarizes basic statistics of the documents we will be experimenting with in the remainder of this work. Number of companies/documents Average number of pages Average number of questions Average number of tokens 50 27 16 60,000 Table 1: Summary Statistics for the call transcript documents used in the present work.',\n",
       " 'Table 1: Summary Statistics for the call transcript documents used in the present work. These call transcripts documents consist of questions and answers between financial analysts and the company representatives for the respective companies, hence, there already exist certain Q&A pairs within these documents along with additional text. We examined the earnings reports within the Nifty50 universe, systematically curated a comprehensive array of randomly selected 400',\n",
       " '5https://huggingface.co/datasets/PatronusAI/financebench-test questions posed during the earnings calls from all the documents, and gathered the exact responses corresponding to these questions. These questions constitute the specific queries articulated by financial analysts to the management during these calls. 4 IMPLEMENTATION DETAILS In this Section, we provide details of implementation of the proposed methodology.',\n",
       " '4.1 Knowledge Graph Construction The initial phase of our approach centers on document preprocessing. We utilize the PyPDFLoader6 to import PDF documents, which are subsequently segmented into manageable chunks using the RecursiveCharacterTextSplitter. This chunking strategy employs a size of 2024 characters with an overlap of 204 characters, ensuring comprehensive coverage while maintaining context across segment boundaries.',\n",
       " 'Following the preprocessing stage, we implement the two-tiered language model chain for content refinement and information extraction. It is not possible to include the exact prompt here due to the limited space, but a baseline prompt can be found in Ref. [18]. Entity Type Companies and Corporations Financial Metrics and Indicators Corporate Executives and Key Personnel Products and Services Examples Official names, abbreviations, informal references Revenue, profit margins, EBITDA',\n",
       " 'Examples Official names, abbreviations, informal references Revenue, profit margins, EBITDA CEOs, CFOs, board members Tangible products and intangible services Geographical Locations Headquarters, operational regions, Corporate Events markets Mergers, launches, earnings calls Legal cases, regulatory compliance acquisitions, product Legal and Regulatory Information Table 2: Entities extracted from earnings call transcripts',\n",
       " 'Table 2 summarizes details on entities extracted from the earning calls transcripts using our prompt based method. Concurrently, LLM identifies relationships between these entities using a curated set of verbs, capturing the nuanced interactions within the corporate narrative. A key improvement in our methodology is the enhanced prompt engineering to generate the structured output format for knowledge triplets',\n",
       " 'Each triplet is represented as a nested list [‚Äôh‚Äô, ‚Äôtype‚Äô, ‚Äôr‚Äô, ‚Äôo‚Äô, ‚Äôtype‚Äô, ‚Äômetadata‚Äô], where ‚Äôh‚Äô and ‚Äôo‚Äô denote the head and object entities respectively, ‚Äôtype‚Äô specifies the entity category, ‚Äôr‚Äô represents the relationship, and ‚Äômetadata‚Äô encapsulates additional contextual information. This format allows for a rich, multidimensional representation of information, facilitating more nuanced downstream analysis.',\n",
       " '6https://python.langchain.com/v0.1/docs/modules/data_connection/document_ loaders/pdf/',\n",
       " 'Our process incorporates several advanced features to enhance the quality and utility of the extracted triplets. Entity disambiguation techniques are employed to consolidate different references to the same entity, improving consistency across the KG. We also prioritize conciseness in entity representation, aiming for descriptions of less than four words where possible, which aids in maintaining a clean and navigable graph structure.',\n",
       " 'The extraction pipeline is applied iteratively to each document chunk, with results aggregated to form a comprehensive knowledge graph representation of the entire document, allowing for scalable processing of large documents while maintaining local context within each chunk. We have added explicit instruction on obtaining metadata following Ref. [8] for both VectorRAG and GraphRAG.',\n",
       " 'Finally, we implement a data persistence strategy, converting the extracted triplets from their initial string format to Python data structures and storing them in a pickle file. This facilitates easy retrieval and further manipulation of the knowledge graph data in subsequent analysis stages.',\n",
       " 'Our methodology represents a significant advancement in automated knowledge extraction from corporate documents. By combining advanced NLP techniques with a structured approach to information representation, we create a rich, queryable knowledge base that captures the complex relationships and key information present in corporate narratives. This approach opens up new possibilities for financial analysis and automated reasoning in the business domain that will be explored further in the future',\n",
       " '',\n",
       " '4.2 VectorRAG Our methodology builds upon the concept of RAG [9] which allows for the creation of a system that can provide context-aware, accurate responses to queries about company financial information, leveraging both the power of large language models and the efficiency of semantic search.',\n",
       " 'At the core of our system is a Pinecone vector database7, which serves as the foundation for our information retrieval process. We employ OpenAI‚Äôs text-embedding-ada-002 model to transform textual data from earnings call transcripts into high-dimensional vector representations. This vectorization process enables semantic similarity searches, significantly enhancing the relevance and accuracy of retrieved information',\n",
       " 'Table 3 provides summary of the configuration of the set up in use for our experiments.',\n",
       " 'LLM LLM Temperature Embedding Model Framework Vector Database Chunk Size Chunk Overlap Maximum Output Tokens Chunks for Similarity Algorithm 20 Number of Context Retrieved 4 GPT-3.5-Turbo 0 text-embedding-ada-002 LangChain Pinecone 1024 0 1024 Table 3: VectorRAG Configuration Sarmah et al.',\n",
       " 'Table 3: VectorRAG Configuration Sarmah et al. The Q&A pipeline is constructed using the LangChain framework8. The begins with a context retrieval step, where we query the Pinecone vector store to obtain the most relevant document chunks for a given question. This retrieval process is fine-tuned with specific filters for quarters, years, and company names, ensuring that the retrieved information is both relevant and current.',\n",
       " 'Following retrieval, we implement a context formatting step that consolidates the retrieved document chunks into a coherent context string. This formatted context serves as the informational basis for the language model‚Äôs response generation',\n",
       " 'We have developed a sophisticated prompt template, that instructs the language model to function as an expert Q&A system, emphasizing the importance of utilizing only the provided context information and avoiding direct references to the context in the generated responses.',\n",
       " 'For the core language processing task, we integrate OpenAI‚Äôs GPT-3.5-turbo model which processes the formatted context and query to generate natural language responses that are informative, coherent, and contextually appropriate.',\n",
       " 'To evaluate the performance of our system, we developed a comprehensive framework that includes the preparation of a custom dataset of question-answer pairs specific to each company‚Äôs earnings call. Our system processes each question in this dataset, generating answers based on the retrieved context. The evaluation results, including the original question, generated answer, retrieved contexts, and ground truth, are compiled into structured formats (CSV and JSON) to facilitate further analysis',\n",
       " 'The outputs generated by our system are stored in both CSV and JSON formats, enabling easy integration with various analysis tools and dashboards. This approach facilitates both quantitative performance metrics and qualitative assessment of the system‚Äôs responses, providing a comprehensive view of its effectiveness.',\n",
       " 'By parameterizing company names, quarters, and years, we can easily adapt the system to different datasets and time periods. This design choice allows for seamless integration of new data and expansion to cover multiple companies and earnings calls. 4.3 GraphRAG For GraphRAG, we developed an Q&A system specifically designed for corporate earnings call transcripts. Our implementation of GraphRAG leverages several key components and techniques:',\n",
       " 'LLM LLM Temperature Framework KG Manipulation Chunk Size Chunk Overlap Number of Triplets Number of nodes Number of edges DFS Depth Table 4: GraphRAG Configuration GPT-3.5-Turbo 0 LangChain Networkx 1024 0 13950 11405 13883 1 7https://www.pinecone.io/ 8https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain \\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction',\n",
       " 'Knowledge Graph Construction:- We begin by constructing a KG from a set of knowledge triplets extracted from corporate documents using the prompt engineering based methodology as described in Section 4.1. These triplets, stored in a pickle file, represent structured information in the form of subject-predicate-object relationships. We use the NetworkxEntityGraph class from the LangChain library to create and manage this graph structure',\n",
       " 'Each triple is added to the graph, which encapsulates the head entity, relation, tail entity, and additional metadata.',\n",
       " 'We implement the Q&A functionality using the GraphQAChain class from LangChain. This chain combines the KG with an LLM (in our case, OpenAI‚Äôs GPT-3.5-turbo) to generate responses. The GraphQAChain traverses the KG to find relevant information and uses the language model to formulate coherent answers based on the retrieved context. A summary of configuration of our LLM models and other libraries used for GraphRAG is shown in Table 4',\n",
       " 'In KG as the information is stored in the form of entities and relationships and there can be multiple relations emanating from one single entity, in this experiment, to extract relevant information from the KG, we employ a depth-first search strategy constrained by a depth of one from the specified entity.',\n",
       " 'To prepare for assessing the performance of our GraphRAG system, we follow the below steps: Dataset Preparation:- We use a pre-generated CSV file containing question-answer pairs specific to the earnings call transcript. Iterative Processing:- For each question in the dataset, we run the GraphQAChain to generate an answer. Result Compilation:- We compile the results, including the original questions, generated answers, retrieved contexts, and ground truth answers, into a structured format.',\n",
       " 'Finally, the evaluation results are saved in both CSV and JSON formats for further analysis and comparison. We then fed these outputs into our RAG evaluation pipeline. For each Q&A pair in our dataset, we compute all three metrics: faithfulness, answer relevance, context precision and context recall.',\n",
       " '4.4 HybridRAG For the proposed HybridRAG technique, upon obtaining all the contextual information from VectorRAG and GraphRAG, we concatenate these contexts to form a unified context utilizing both techniques. This combined context is then fed into the answer generator model to produce a response. The context used for response generation is relatively larger, which affects the precision of the generated response',\n",
       " 'The context from VectorRAG is appended first, followed by the context from GraphRAG. Consequently, the precision of the generated answer depends on the source context. If the answer is generated from the GraphRAG context, it will have lower precision, as the GraphRAG context is added last in the sequence of contexts provided to the answer generator model, and vice versa.',\n",
       " '5 RESULTS We evaluate both the retrieval and generation parts of RAG for the three different RAG pipelines. Evaluating the RAG outputs is also an active area of research there is no standard tool which is universally accepted as of yet, though we use a currently popular framework RAGAS[30] to evaluate the three RAG pipelines in the present work where we have modified them a bit to make more precise comparisons.',\n",
       " 'The results of our comparative analysis reveal notable differences in performance among VectorRAG, GraphRAG, and HybridRAG approaches as summarized in Table 5. In terms of Faithfulness, GraphRAG and HybridRAG demonstrated superior performance, both achieving a score of 0.96, while VectorRAG trailed slightly with a score of 0.94. Answer relevancy scores varied across the methods, with HybridRAG outperforming the others at 0.96, followed by VectorRAG at 0.91, and GraphRAG at 0.89',\n",
       " '96, followed by VectorRAG at 0.91, and GraphRAG at 0.89. Context precision was highest for GraphRAG at 0.96, significantly surpassing VectorRAG (0.84) and HybridRAG (0.79). However, in context recall, both VectorRAG and HybridRAG achieved perfect scores of 1, while GraphRAG lagged behind at 0.85.',\n",
       " 'Overall, these results suggest that GraphRAG offers improvements over VectorRAG, particularly in faithfulness and context precision. Furthermore, HybridRAG emerges as the most balanced and effective approach, outperforming both VectorRAG and GraphRAG in key metrics such as faithfulness and answer relevancy, while maintaining high context recall.',\n",
       " 'The relatively lower context precision observed for HybridRAG (0.79) can be attributed to its unique approach of combining contexts from both VectorRAG and GraphRAG methods. While this integration allows for more comprehensive information retrieval, it also introduces additional content that may not align precisely with the ground truth, thus affecting the context precision metric',\n",
       " 'Despite this trade-off, HybridRAG‚Äôs superior performance in faithfulness, answer relevancy, and context recall underscores its effectiveness. When considering the overall evaluation metrics, HybridRAG emerges as the most promising approach, balancing high-quality answers with comprehensive context retrieval.',\n",
       " 'Overall GraphRAG performs better in extractive questions compared to VectorRAG. And VectorRAG does better in abstractive questions where information is not explicitly mentioned in the raw data. And also GraphRAG sometimes fails to answer questions correctly whenever there is no entity explicitly mentioned in the question. So HybridRAG does a good job overall, as whenever VectorRAG fails to fetch correct context in extractive questions it falls back to GraphRAG to generate the answer',\n",
       " 'And whenever GraphRAG fails to fetch correct context in abstractive questions it falls back to VectorRAG to generate the answer.',\n",
       " 'VectorRAG GraphRAG HybridRAG 0.96 0.89 0.96 0.85 0.96 0.96 0.79 1 0.94 0.91 0.84 1 F AR CP CR Table 5: Performance Metrics for Different RAG Pipelines. Here, F, AR, CP and CR refer to Faithfulness, Answer Relevence, Context Precision and Context Recall. 6 CONCLUSION AND FUTURE DIRECTIONS Among the current approaches to mitigate issues regarding information extraction from external documents that were not part of',\n",
       " 'training data for the LLM, Retrieval Augmented Generation (RAG) techniques have emerged as the most popular ones that aim to improve the performance of LLMs by incorporating relevant retrieval mechanisms. RAG methods enhance the LLMs‚Äô capabilities by retrieving pertinent documents or text to provide additional context during the generation process. However, these approaches encounter significant limitations when applied to the specialized and intricate domain of financial documents',\n",
       " 'Furthermore, the quality of the retrieved context from a vast and heterogeneous corpus can be inconsistent, leading to inaccuracies and incomplete analyses. These challenges highlight the need for more sophisticated methods that can effectively integrate and process the detailed and domainspecific information found in financial documents, ensuring more reliable and accurate outputs for informed decision-making.',\n",
       " 'In the present work, we have introduced a novel approach that significantly advances the field of information extraction from financial documents through the development of a hybrid RAG system. This system, called HybridRAG, which integrates the strengths of both Knowledge Graphs (KGs) and advanced language models, represents a leap forward in our ability to extract and interpret complex information from unstructured financial texts',\n",
       " 'The hybrid RAG system, by combining traditional vector-based RAG and KGbased RAG, has shown superior performance in terms of retrieval accuracy and answer generation, marking a pivotal step towards more effective financial analysis tools.',\n",
       " 'Through a comparative analysis using objecive evaluation metrics, we have highlighted the distinct performance advantages of the HybridRAG approach over its vector-based and KG-based counterparts. The HybridRAG system excels in faithfulness, answer relevancy, and context recall, demonstrating the benefits of integrating contexts from both VectorRAG and GraphRAG methods, despite potential trade-offs in context precision.',\n",
       " 'The implications of this research extend beyond the immediate realm of financial analysis. By developing a system capable of understanding and responding to nuanced queries about complex financial documents, we pave the way for more sophisticated AI-assisted financial decision-making tools that could potentially democratize access to financial insights, allowing a broader range of stakeholders to engage with and understand financial information',\n",
       " 'Future directions for this research include expanding the system to handle multi-modal inputs, incorporating numerical data analysis capabilities, and developing more sophisticated evaluation metrics that capture the nuances of financial language and the accuracy of numerical information in the responses. Additionally, exploring the integration of this system with real-time financial data streams could further enhance its utility in dynamic financial environments.',\n",
       " '7 ACKNOWLEDGEMENT The views expressed here are those of the authors alone and not of BlackRock, Inc or NVIDIA. We are grateful to Emma Lind for her invaluable support for this collaboration. REFERENCES [1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation Sarmah et al. [3] Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: A pretrained language model for financial communications. arXiv preprint arXiv:2006.08097, 2020.',\n",
       " '[4] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023. [5] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qingsong Wen, and Stefan Zohren. A survey of large language models for financial applications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903, 2024.',\n",
       " '[6] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, and Tianming Liu. Revolutionizing finance with llms: An overview of applications and insights, 2024.',\n",
       " '[7] Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv preprint arXiv:2305.18703, 2023.',\n",
       " '[8] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. Towards reducing hallucination in extracting information from financial reports using large language models. In Proceedings of the Third International Conference on AI-ML Systems, pages 1‚Äì5, 2023.',\n",
       " '[9] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459‚Äì9474, 2020. [10] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2021.',\n",
       " '[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. arXiv preprint Realm: Retrieval-augmented language model pre-training. arXiv:2002.08909, 2020. [12] Antonio Jose Jimeno Yepes et al. Financial report chunking for effective retrieval augmented generation. arXiv preprint arXiv:2402.05131, 2024. [13] SuperAcc. Retrieval-augmented generation on financial statements. SuperAcc Insights, 2024.',\n",
       " '[13] SuperAcc. Retrieval-augmented generation on financial statements. SuperAcc Insights, 2024. [14] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494‚Äì514, 2021. [15] Heiko Paulheim. Knowledge graphs: State of the art and future directions. Semantic Web Journal, 10(4):1‚Äì20, 2017.',\n",
       " 'Semantic Web Journal, 10(4):1‚Äì20, 2017. [16] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D‚ÄôAmato, Gerard de Melo, Claudio Guti√©rrez, and Maria ... Maleshkova. Knowledge graphs. arXiv preprint arXiv:2003.02320, 2021. [17] Lisa Ehrlinger and Wolfram W√∂√ü. Towards a definition of knowledge graphs. In SEMANTiCS (Posters, Demos, SuCCESS), pages 1‚Äì4, 2016.',\n",
       " 'SEMANTiCS (Posters, Demos, SuCCESS), pages 1‚Äì4, 2016. [18] Xiaohui Victor Li and Francesco Sanna Passino. Findkg: Dynamic knowledge graphs with large language models for detecting global trends in financial markets. arXiv preprint arXiv:2407.10909, 2024.',\n",
       " '[19] Shourya De, Anima Aggarwal, and Anna Cinzia Squicciarini. Financial knowledge graphs: A novel approach to empower data-driven financial applications. In IEEE International Conference on Big Data (Big Data), pages 1311‚Äì1320, 2018. [20] Marina Petrova and Birgit Reinwald. Knowledge graphs in finance: Applications and opportunities. Journal of Financial Data Science, 2(2):10‚Äì19, 2020. [21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data',\n",
       " '[21] Wei Liu, Jun Zhang, and Li Pan. Utilizing knowledge graphs for financial data integration and analysis. Data Science Journal, 18(1):1‚Äì15, 2019. [22] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.',\n",
       " '[23] Xuchen Yao, Yanan Sun, Zhen Huang, and Dong Li. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2101.07554, 2021. [24] Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao. Graph-based retrievalaugmented generation for open-domain question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3098‚Äì3106, 2022.',\n",
       " '[25] Bill Yuchen Lin, Yuanhe Liu, Ming Shen, and Xiang Ren. Kgpt: Knowledgegrounded pre-training for data-to-text generation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 710‚Äì724, 2020. [26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.',\n",
       " '[27] Tyler Procko. Graph retrieval-augmented generation for large language models: of word representations in vector space. 2013. A survey. Available at SSRN, 2024. [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.',\n",
       " '[28] Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):1‚Äì62, 2023. \\x0cHybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction [29] Ishani Mondal, Yufang Hou, and Charles Jochim. End-to-end nlp knowledge graph construction. arXiv preprint arXiv:2106.01167, 2021.',\n",
       " '[30] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023. [31] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122, 2021.',\n",
       " '[32] Macedo Maia, Siegfried Handschuh, Andr√© Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www‚Äô18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018, pages 1941‚Äì1942, 2018.',\n",
       " 'web conference 2018, pages 1941‚Äì1942, 2018. [33] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624, 2021.',\n",
       " '[34] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang. Fintextqa: A dataset for long-form financial question answering. arXiv preprint arXiv:2405.09980, 2024. [35]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Internship\\Chat_With_Docs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_embeddings = model.encode(cleaned_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 384)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 2 0 2 g u A 9 ] L C . s c [ 1 v 8 4 9 4 0 . ...</td>\n",
       "      <td>[-0.019694823771715164, 0.03228660300374031, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunil Patel supatel@nvidia.com NVIDIA Santa Cl...</td>\n",
       "      <td>[0.001957190688699484, -0.0019113713642582297,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABSTRACT Extraction and interpretation of intr...</td>\n",
       "      <td>[-0.03259172663092613, 0.024141255766153336, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We introduce a novel approach based on a combi...</td>\n",
       "      <td>[-0.0541795939207077, 0.0531788133084774, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Using experiments on a set of financial earnin...</td>\n",
       "      <td>[-0.03212740644812584, 0.04043416306376457, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  4 2 0 2 g u A 9 ] L C . s c [ 1 v 8 4 9 4 0 . ...   \n",
       "1  Sunil Patel supatel@nvidia.com NVIDIA Santa Cl...   \n",
       "2  ABSTRACT Extraction and interpretation of intr...   \n",
       "3  We introduce a novel approach based on a combi...   \n",
       "4  Using experiments on a set of financial earnin...   \n",
       "\n",
       "                                          Embeddings  \n",
       "0  [-0.019694823771715164, 0.03228660300374031, -...  \n",
       "1  [0.001957190688699484, -0.0019113713642582297,...  \n",
       "2  [-0.03259172663092613, 0.024141255766153336, -...  \n",
       "3  [-0.0541795939207077, 0.0531788133084774, -0.0...  \n",
       "4  [-0.03212740644812584, 0.04043416306376457, -0...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "assert len(chunk_embeddings) == len(cleaned_chunk)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Text\" : cleaned_chunk,\n",
    "        \"Embeddings\" : [row.tolist() for row in chunk_embeddings]\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is hybridRAG?\"\n",
    "\n",
    "embedded_query = model.encode(clean_chunk(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity(a,b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "\n",
    "    dot_product = np.dot(a,b)\n",
    "\n",
    "    mag_a = np.linalg.norm(a)\n",
    "    mag_b = np.linalg.norm(b)\n",
    "\n",
    "    similarity = dot_product / (mag_a * mag_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5949147382728737\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(embedded_query,df[\"Embeddings\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"similarity\"] = df[\"Embeddings\"].apply(lambda x : cosine_similarity(x, embedded_query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Embeddings</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 2 0 2 g u A 9 ] L C . s c [ 1 v 8 4 9 4 0 . ...</td>\n",
       "      <td>[-0.019694823771715164, 0.03228660300374031, -...</td>\n",
       "      <td>0.594915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunil Patel supatel@nvidia.com NVIDIA Santa Cl...</td>\n",
       "      <td>[0.001957190688699484, -0.0019113713642582297,...</td>\n",
       "      <td>0.462932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABSTRACT Extraction and interpretation of intr...</td>\n",
       "      <td>[-0.03259172663092613, 0.024141255766153336, -...</td>\n",
       "      <td>0.541232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We introduce a novel approach based on a combi...</td>\n",
       "      <td>[-0.0541795939207077, 0.0531788133084774, -0.0...</td>\n",
       "      <td>0.603789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Using experiments on a set of financial earnin...</td>\n",
       "      <td>[-0.03212740644812584, 0.04043416306376457, -0...</td>\n",
       "      <td>0.606146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  4 2 0 2 g u A 9 ] L C . s c [ 1 v 8 4 9 4 0 . ...   \n",
       "1  Sunil Patel supatel@nvidia.com NVIDIA Santa Cl...   \n",
       "2  ABSTRACT Extraction and interpretation of intr...   \n",
       "3  We introduce a novel approach based on a combi...   \n",
       "4  Using experiments on a set of financial earnin...   \n",
       "\n",
       "                                          Embeddings  similarity  \n",
       "0  [-0.019694823771715164, 0.03228660300374031, -...    0.594915  \n",
       "1  [0.001957190688699484, -0.0019113713642582297,...    0.462932  \n",
       "2  [-0.03259172663092613, 0.024141255766153336, -...    0.541232  \n",
       "3  [-0.0541795939207077, 0.0531788133084774, -0.0...    0.603789  \n",
       "4  [-0.03212740644812584, 0.04043416306376457, -0...    0.606146  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df.sort_values(\"similarity\", ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Embeddings</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>In the present work, we propose a combination ...</td>\n",
       "      <td>[-0.04629891738295555, -0.014486481435596943, ...</td>\n",
       "      <td>0.756713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>The results of our comparative analysis reveal...</td>\n",
       "      <td>[-0.004982789978384972, -0.03970677778124809, ...</td>\n",
       "      <td>0.756547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Overall GraphRAG performs better in extractive...</td>\n",
       "      <td>[-0.025407111272215843, -0.03301198408007622, ...</td>\n",
       "      <td>0.751247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Despite this trade-off, HybridRAG‚Äôs superior p...</td>\n",
       "      <td>[-0.025110999122262, 0.030811332166194916, -0....</td>\n",
       "      <td>0.740144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Overall, these results suggest that GraphRAG o...</td>\n",
       "      <td>[-0.02828267589211464, -0.038765255361795425, ...</td>\n",
       "      <td>0.721592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "21   In the present work, we propose a combination ...   \n",
       "110  The results of our comparative analysis reveal...   \n",
       "115  Overall GraphRAG performs better in extractive...   \n",
       "114  Despite this trade-off, HybridRAG‚Äôs superior p...   \n",
       "112  Overall, these results suggest that GraphRAG o...   \n",
       "\n",
       "                                            Embeddings  similarity  \n",
       "21   [-0.04629891738295555, -0.014486481435596943, ...    0.756713  \n",
       "110  [-0.004982789978384972, -0.03970677778124809, ...    0.756547  \n",
       "115  [-0.025407111272215843, -0.03301198408007622, ...    0.751247  \n",
       "114  [-0.025110999122262, 0.030811332166194916, -0....    0.740144  \n",
       "112  [-0.02828267589211464, -0.038765255361795425, ...    0.721592  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the present work, we propose a combination of VectorRAG and GraphRAG, called HybridRAG, to retrieve the relevant information from external documents for a given query to the LLM that brings advantages of both the RAGs together to provide demonstrably more accurate answers to the queries.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"Text\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = results[\"Text\"].head(3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client(\"3Ey34YJTJS112veQg08hrjU8vmch67EhBWeiWBNU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the summary of this paper?\"\n",
    "embedded_query = model.encode(clean_chunk(query))\n",
    "df[\"similarity\"] = df[\"Embeddings\"].apply(lambda x : cosine_similarity(x, embedded_query))\n",
    "results = df.sort_values(\"similarity\", ascending= False)\n",
    "top_k = results[\"Text\"].head(3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Embeddings</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Due to the complexities in analyzing financial...</td>\n",
       "      <td>[-0.007800274528563023, 0.021058620885014534, ...</td>\n",
       "      <td>0.651927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>\"Consider the given context and following stat...</td>\n",
       "      <td>[-0.061397381126880646, 0.048492200672626495, ...</td>\n",
       "      <td>0.649732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Furthermore, the quality of the retrieved cont...</td>\n",
       "      <td>[0.01591600477695465, 0.0022000079043209553, -...</td>\n",
       "      <td>0.646635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Table 1: Summary Statistics for the call trans...</td>\n",
       "      <td>[-0.0033405821304768324, -0.00464419461786747,...</td>\n",
       "      <td>0.643603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>A schematic diagram of the retrieval methodolo...</td>\n",
       "      <td>[0.021049920469522476, -0.019680125638842583, ...</td>\n",
       "      <td>0.634759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "14   Due to the complexities in analyzing financial...   \n",
       "58   \"Consider the given context and following stat...   \n",
       "119  Furthermore, the quality of the retrieved cont...   \n",
       "76   Table 1: Summary Statistics for the call trans...   \n",
       "40   A schematic diagram of the retrieval methodolo...   \n",
       "\n",
       "                                            Embeddings  similarity  \n",
       "14   [-0.007800274528563023, 0.021058620885014534, ...    0.651927  \n",
       "58   [-0.061397381126880646, 0.048492200672626495, ...    0.649732  \n",
       "119  [0.01591600477695465, 0.0022000079043209553, -...    0.646635  \n",
       "76   [-0.0033405821304768324, -0.00464419461786747,...    0.643603  \n",
       "40   [0.021049920469522476, -0.019680125638842583, ...    0.634759  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the formatted context\n",
    "formatted_input = \"Context:\\n\"\n",
    "for idx, result in enumerate(top_k, 1):\n",
    "    formatted_input += f\"{idx}. {result}\\n\"\n",
    "\n",
    "full_prompt = f\"\"\"\n",
    "You are given a context based on multiple sources. Your task is to answer the following question based on the provided context. \n",
    "If the context does not contain relevant information or does not match the query, please respond with \"I cannot answer based on the given context.\"\n",
    "\n",
    "{formatted_input}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Instructions:\n",
    "- Use your knowledge to check which context is most relevant to the question being asked.\n",
    "- Provide a concise, clear, and relevant answer based on the context.\n",
    "- Keep your answer short and dont ask additional questions.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given a context based on multiple sources. Your task is to answer the following question based on the provided context. \n",
      "If the context does not contain relevant information or does not match the query, please respond with \"I cannot answer based on the given context.\"\n",
      "\n",
      "Context:\n",
      "1. Table 1: Summary Statistics for the call transcript documents used in the present work. These call transcripts documents consist of questions and answers between financial analysts and the company representatives for the respective companies, hence, there already exist certain Q&A pairs within these documents along with additional text. We examined the earnings reports within the Nifty50 universe, systematically curated a comprehensive array of randomly selected 400\n",
      "2. Table 1 summarizes basic statistics of the documents we will be experimenting with in the remainder of this work. Number of companies/documents Average number of pages Average number of questions Average number of tokens 50 27 16 60,000 Table 1: Summary Statistics for the call transcript documents used in the present work.\n",
      "3. Due to the complexities in analyzing financial documents, the quality of the LLM retrieved-context from a vast and heterogeneous corpus can be inconsistent, leading to inaccuracies and incomplete analyses. These challenges demonstrate the need for more sophisticated methods that can effectively integrate and process the detailed and domain-specific information found in financial documents, ensuring more reliable and accurate results for informed decision-making.\n",
      "\n",
      "\n",
      "Question:\n",
      "What is the summary of this paper?\n",
      "\n",
      "Instructions:\n",
      "- Use your knowledge to check which context is most relevant to the question being asked.\n",
      "- Provide a concise, clear, and relevant answer based on the context.\n",
      "- Keep your answer short and dont ask additional questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = co.generate(\n",
    "    model=\"command\",\n",
    "    prompt=full_prompt,\n",
    "    max_tokens=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This paper seems to be examining the need for a more complex analysis of LLM-generated financial contexts, amidst a call for more sophisticated methods that can effectively integrate and process the detailed and domain-specific information found in financial documents, ensuring more reliable and accurate results for informed decision-making in the future. '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.generations[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
